{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as snb\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from autograd import value_and_grad\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def add_colorbar(im, fig, ax):\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "\n",
    "snb.set_style('darkgrid')\n",
    "snb.set_theme(font_scale=1.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02477 Bayesian Machine Learning: Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to\n",
    "- learn how to sample from a multivariate Gaussian distribution\n",
    "- become familiar with covariance functions, especially stationary covariance functions such as the squared exponential and Matérn family and to understand their properties and hyperparameters\n",
    "- become familiar with Gaussian processes and learn to use them as prior distribution for latent functions in regression settings\n",
    "\n",
    "\n",
    "**Content**\n",
    "\n",
    "- Part 1: Sampling from a multivariate Gaussian distribution\n",
    "- Part 2: Stationary covariance functions\n",
    "- Part 3: Non-linear regression using Gaussian processes\n",
    "- Part 4: Hyperparameter optimization using the marginal likelihood\n",
    "- Part 5: Analysing the bike sharing data set\n",
    "\n",
    "\n",
    "**Note**: The exercise contains several **discussion questions**, which are questions, where are supposed to actively experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs. Instead of proving the full description for every discussion question, we simply tag it with: [**Discussion question**] after the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we worked with parametric models such as linear regression and logistic regression, we assigned prior distributions on the weights $\\mathbf{w}$ of the models. In contrast, we can think of Gaussian processes as prior distributions directly on the function space\n",
    "\n",
    "$$\\begin{align*}\n",
    "f(\\mathbf{x}) \\sim \\mathcal{GP}\\left(m(\\mathbf{x}) \\, , \\, k\\left(\\mathbf{x}, \\mathbf{x}'\\right)\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "where $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}$ and $\\mathbf{m}(\\mathbf{x}) = \\mathbb{E}\\left[f(\\mathbf{x})\\right]$, and $\\mathbf{k}(\\mathbf{x}, \\mathbf{x}') = \\mathbb{E}\\left[\\left(f(\\mathbf{x})-m(\\mathbf{x})\\right)\\left(f(\\mathbf{x'})-m(\\mathbf{x'})\\right)\\right]$ are the **mean function** and **covariance function**, respectively, for $\\mathbf{x}, \\mathbf{x}' \\in \\mathbb{R}$.\n",
    "\n",
    "\n",
    "A Gaussian process is completely specified by its mean and covariance function. Most often we take the mean function to be zero, $m(\\mathbf{x}) = 0$, because we don't have any prior information on the mean function:\n",
    "\n",
    "$$\\begin{align*}\n",
    "f(\\mathbf{x}) \\sim \\mathcal{GP}\\left(0 \\, , \\, k\\left(\\mathbf{x}, \\mathbf{x}'\\right)\\right), \\tag{1}\n",
    "\\end{align*}$$\n",
    "\n",
    "The **covariance function** (or kernel function) $k(\\mathbf{x}, \\mathbf{x}')$ thus completely determines the a priori behavior of the latent function $f$. Specifically, the covariance function determines the covariance of the latent function $f$ when evaluated at two different inputs, i.e.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{cov}[f(\\mathbf{x}), f(\\mathbf{x}')] = k(\\mathbf{x}, \\mathbf{x}'). \\tag{2}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "The **squared exponential** covariance function is given by\n",
    "\n",
    "\\begin{align*} \n",
    "    k_{\\text{SE}}(\\mathbf{x}_n, \\mathbf{x}_m) = \\kappa^2 \\exp\\left(-\\frac{\\|\\mathbf{x}_n - \\mathbf{x}_m\\|^2_ 2}{2\\ell^2}\\right), \\tag{3}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\kappa > 0$ and $\\ell > 0$ are hyperparameters of the kernel. This specific covariance function is perhaps the most common covariance function used in statistics and machine learning. It is also known as the radial basis function kernel, the gaussian kernel, or the exponentiated quadratic kernel. We will also study two covariance functions from the Matérn family:\n",
    "\n",
    "$$\\begin{align*}\n",
    "k_{\\text{Matern12}}(\\mathbf{x}_n, \\mathbf{x}_m) &= \\kappa^2 \\exp\\left(-\\frac{||\\mathbf{x}_n- \\mathbf{x}_m||}{\\ell}\\right)\\\\\n",
    "k_{\\text{Matern32}}(\\mathbf{x}_n, \\mathbf{x}_m) &= \\kappa^2 \\left(1 + \\frac{\\sqrt{3}||\\mathbf{x}_n - \\mathbf{x}_m||}{\\ell}\\right)\\exp\\left(-\\frac{\\sqrt{3}||\\mathbf{x}_n- \\mathbf{x}_m||}{\\ell}\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:  Sampling from a multivariate Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To become familiar with covariance functions and understand its properties, we will generate and visualize samples from Gaussian process distributions. Therefore, our first order of business will be to implement a function for generating samples from a Gaussian process prior.\n",
    "\n",
    "\n",
    "Let $\\mathbf{X} = \\left\\lbrace \\mathbf{x}_n \\right\\rbrace_{n=1}^N $ be a set of points in the real line, i.e. $\\mathbf{x}_n \\in \\mathbb{R}^D$, and let $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}$ be a function. Let $f_n \\equiv f(x_n) \\in \\mathbb{R}$ be the value of the function $f$ evaluated at $x_n \\in \\mathbb{R}$. Furthermore, let $\\mathbf{f} \\equiv \\left[f_1, f_2, \\dots, f_N\\right] \\in \\mathbb{R}^{N \\times 1}$ be the vector of function values for each of the points in $\\mathbf{X}$, then the Gaussian process prior for $\\mathbf{f}$ becomes\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{f} \\sim \\mathcal{N}\\left(\\mathbf{0}, \\mathbf{K}\\right), \\tag{4}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{K} \\in \\mathbb{R}^{N \\times N}$ is the kernel matrix satisfying $\\mathbf{K}_{nm} = k(\\mathbf{x}_n ,\\mathbf{x}_m)$, where $\\mathbf{x}_n$ refers to the $n$'th input point, i.e. the $i$'th entry of $\\mathbf{X}$.\n",
    "\n",
    "The **Cholesky decomposition** is sometimes refered to as *matrix square root* and it is really handy matrix decomposition for sampling from a multivariate Gaussian distribution. The Cholesky decomposition of the kernel matrix is given by $\\mathbf{K} = \\mathbf{L}\\mathbf{L}^T$, where $\\mathbf{L} \\in \\mathbb{R}^{N \\times N}$ is a lower triangular matrix.\n",
    "\n",
    "The idea is to generate samples from standardized Gaussian distributions, and then transform those samples into the desired distribution. Let $\\mathbf{z} \\in \\mathbb{R}^{N \\times 1}$ be a sample from a standardized multivariate Gaussian distribution, i.e. $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_n)$, where $\\mathbf{I}_n$ is the identity matrix of size $N \\times N$.\n",
    "\n",
    "**Task 1.1**: Show that if $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_n)$ and $\\mathbf{m} \\in \\mathbb{R}^{N}$, then $\\mathbf{f} = \\mathbf{L}\\mathbf{z} + \\mathbf{m} \\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{K})$. \n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All real-valued symmetric and positive definite matrices have a unique Cholesky decomposition. However, due to finite precision in computers, the covariance matrices used in Gaussian processes can sometimes have negative eigenvalues in practice and hence fail to be positive definite. A common trick is to add a small positive number $\\epsilon > 0$ to the diagonal of the covariance matrix, which will increase the eigenvalues of the covariance matrix $\\mathbf{K}$ and hence change the sign of a potential small negative eigenvalue. Hence, in practice we construct the covariance matrix as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{K}_{nm}  = k(\\mathbf{x}_n, \\mathbf{x}_m) + \\epsilon \\delta(\\mathbf{x}_n-\\mathbf{x}_m),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\delta(\\cdot)$ is **Kronenecker's delta function** defined by \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\delta(\\mathbf{x}_n-\\mathbf{x}_m) = \\begin{cases} 1 & \\text{if}\\quad \\mathbf{x}_n=\\mathbf{x}_m \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We often say we add **jitter** to the covariance matrix. $\\epsilon$ should be as small as positive to avoid affecting the results, e.g. a typical number is $\\epsilon \\approx 10^{-8}$. If $\\epsilon$ needs to be significantly bigger, then it is likely that there are some other implementation issues, which should be addressed.\n",
    "\n",
    "**Task 1.2**: **(optional)** Can you prove that adding jitter increases the eigenvalues of the resulting matrix?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Task 1.3**: Complete the implementation of the function `generate_samples` below for sampling from a multivariate Gaussian using the method described above. \n",
    "\n",
    "*Hints: The function np.linalg.cholesky is handy for computing the Cholesky decomposition of a matrix. You need to add the jitter before computing the Cholesky decomposition.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(m, K, num_samples, jitter=0):\n",
    "    \"\"\" returns M samples from an Gaussian process with mean m and kernel matrix K. The function generates num_samples of z ~ N(0, I) and transforms them into f  ~ N(m, K) via the Cholesky factorization.\n",
    "\n",
    "    \n",
    "    arguments:\n",
    "        m                -- mean vector (shape (N,))\n",
    "        K                -- kernel matrix (shape NxN)\n",
    "        num_samples      -- number of samples to generate (positive integer)\n",
    "        jitter           -- amount of jitter (non-negative scalar)\n",
    "    \n",
    "    returns \n",
    "        f_samples        -- a numpy matrix containing the samples of f (shape N x num_samples)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    ##############################################\n",
    "    # Your solution goes here\n",
    "    ##############################################\n",
    "    ##############################################\n",
    "    # End of solution\n",
    "    ##############################################\n",
    "\n",
    "    # sanity check of dimensions\n",
    "    assert f_samples.shape == (len(K), num_samples), f\"The shape of f_samples appears wrong. Expected shape ({len(K)}, {num_samples}), but the actual shape was {f_samples.shape}. Please check your code. \"\n",
    "    return f_samples\n",
    "\n",
    "\n",
    "# sanity check of implementation\n",
    "num_samples = 100000\n",
    "m = np.array([np.pi, np.sqrt(2)])\n",
    "V = np.array([[0.123, -0.05], [-0.05, 0.123]])\n",
    "f_samples = generate_samples(m, V, num_samples)\n",
    "assert np.linalg.norm(np.mean(f_samples, 1)- m) < 1e-2, \"The mean of f_samples appears wrong. Please check your code\"\n",
    "assert np.linalg.norm(np.cov(f_samples) - V) < 1e-2, \"The covariance of f_samples appears wrong. Please check your code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this function in the next section to generate samples from a Gaussian process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:  Stationary covariance functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will dive deeper into the covariance functions and their properties. We will focus on so-called **stationary** covariance functions.\n",
    "\n",
    "The squared exponential kernel is an example of a **stationary** kernel, which means that the covariance function only depends on the difference between two points, i.e. $\\tau = \\mathbf{x}_n - \\mathbf{x}_m$ for two points in the input space $\\mathbf{x}_n, \\mathbf{x}_m \\in \\mathbb{R}^D$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    k_{\\text{SE}}(\\mathbf{x}_n, \\mathbf{x}_m) = \\kappa^2 \\exp\\left(-\\frac{\\|\\mathbf{x}_n - \\mathbf{x}_m\\|^2_ 2}{2\\ell^2}\\right) = \\kappa^2 \\exp\\left(-\\frac{\\|\\tau\\|^2_ 2}{2\\ell^2}\\right) = k(\\tau).\n",
    "\\end{align*}$$\n",
    "\n",
    " Furthermore, if the covariance function only depends on the distance between any two inputs, i.e. $||\\tau||_2 = ||\\mathbf{x}_n - \\mathbf{x}_m||_2$, then the covariance function is also said to be **isotropic** (isotropic is a fancy word for rotation invariance). For the squared exponential kernel we can write:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    k_{\\text{SE}}(\\mathbf{x}_n, \\mathbf{x}_m) = k(||\\tau||) =  \\kappa^2 \\exp\\left(-\\frac{\\|\\tau\\|^2_ 2}{2\\ell^2}\\right), \\tag{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "which shows that the squared exponential kernel is both **stationary** and **isotropic** since it can be written as a function $\\|\\tau\\|$. Note that we in the above overload and abuse the notation of $k$ a bit.\n",
    "\n",
    "Consider the following four covariance functions: the linear kernel, the Matern $\\frac12$ kernel, the Matern $\\frac32$ kernel and the so-called automatic relevance determination (ARD) kernel given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "k_{\\text{Linear}}(\\mathbf{x}_n, \\mathbf{x}_m) &= \\alpha^{-1} \\mathbf{x}^T_m \\mathbf{x}_n\\\\\n",
    "k_{\\text{Matern12}}(\\mathbf{x}_n, \\mathbf{x}_m) &= \\kappa^2 \\exp\\left(-\\frac{||\\mathbf{x}_n- \\mathbf{x}_m||}{\\ell}\\right)\\\\\n",
    "k_{\\text{Matern32}}(\\mathbf{x}_n, \\mathbf{x}_m) &= \\kappa^2 \\left(1 + \\frac{\\sqrt{3}||\\mathbf{x}_n - \\mathbf{x}_m||}{\\ell}\\right)\\exp\\left(-\\frac{\\sqrt{3}||\\mathbf{x}_n- \\mathbf{x}_m||}{\\ell}\\right)\\\\\n",
    "k_{\\text{ARD}}(\\mathbf{x}_n, \\mathbf{x}_m) &= \\kappa^2 \\exp\\left(-\\frac12 \\sum_{i=1}^D \\frac{(\\mathbf{x}_{n,i} -\\mathbf{x}_{m,i})^2}{\\ell_i}\\right)  = \\kappa^2 \\exp\\left(-\\frac12 (\\mathbf{x}_n- \\mathbf{x}_m)^T \\mathbf{L} \\left(\\mathbf{x}_n- \\mathbf{x}_m)\\right)\\right),  \n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\mathbf{L} \\in \\mathbf{R}^{D \\times D}$ is a diagonal matrix $\\mathbf{L} = \\text{diag}(\\ell^2_1, \\ell^2_2, \\dots, \\ell^2_D)$. The ARD-kernel can be seen as a generalization of the squared exponential kernel, where each dimension of $\\mathbf{x}$ has a separate characteristic lengthscale. \n",
    "\n",
    "**Task 2.1**: Which of the four kernels above are stationary kernels and which isotropic kernels?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement the squared exponential kernel, the Matérn $\\frac{1}{2}$, and the Matérn $\\frac{3}{2}$ kernel. The and the cell below provide an implementation of these kernels expressed as a function norm of the distance of the inputs, $||\\tau|| = ||\\mathbf{x}_n - \\mathbf{x}_m||$, e.g. \n",
    "\n",
    "$$\\begin{align*}\n",
    "k_{\\text{SE}}(||\\tau||) = \\kappa^2 \\exp\\left(-\\frac{\\|\\tau\\|^2_ 2}{2\\ell^2}\\right).\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the code below tau represents the distance between to input points, i.e. tau = ||x_n - x_m||.\n",
    "def squared_exponential(tau, kappa, lengthscale):\n",
    "    return kappa**2*np.exp(-0.5*tau**2/lengthscale**2)\n",
    "\n",
    "def matern12(tau, kappa, lengthscale):\n",
    "    return kappa**2*np.exp(-tau/lengthscale)\n",
    "\n",
    "def matern32(tau, kappa, lengthscale):\n",
    "    return kappa**2*(1 + np.sqrt(3)*tau/lengthscale)*np.exp(-np.sqrt(3)*tau/lengthscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you are given a vector $\\mathbf{X} \\in \\mathbb{R}^{N \\times 1}$ of $N = 1000$ points on the real line. The points are sorted and equidistantly distributed in the interval $\\left[-6, 6\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to construct the kernel matrices, all we need to do now is to implement some code for evaluating all the pairwise distances, i.e. $||\\mathbf{x}_n - \\mathbf{x}_m||$ for all pairs of input points, and then feed these distances to the covariance function above along with values for the hyperparameters. This \"two-step\" implementation makes it easy to switch between different covariance functions. \n",
    "\n",
    "**Task 2.2**: Complete the implementation of the class *StationaryIsotropicKernel* below.\n",
    "\n",
    "*Hints: The function `construct_kernel` contain three steps:*\n",
    "\n",
    "1) *compute all the pairwise distances between `X1` and `X2`,*\n",
    "\n",
    "2) *feed these distance to the covariance function `kernel_fun`,*\n",
    "\n",
    "3) *add jitter if needed*\n",
    "\n",
    "- *You can either compute the pairwise distances using for-loops or using a vectorized solution. Using for-loops is a bit slower, but might be easier to understand and debug*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StationaryIsotropicKernel(object):\n",
    "\n",
    "    def __init__(self, kernel_fun, kappa=1., lengthscale=1.0):\n",
    "        \"\"\"\n",
    "            the argument kernel_fun must be a function of three arguments kernel_fun(||tau||, kappa, lengthscale), e.g. \n",
    "            squared_exponential = lambda tau, kappa, lengthscale: kappa**2*np.exp(-0.5*tau**2/lengthscale**2)\n",
    "        \"\"\"\n",
    "        self.kernel_fun = kernel_fun\n",
    "        self.kappa = kappa\n",
    "        self.lengthscale = lengthscale\n",
    "\n",
    "    def contruct_kernel(self, X1, X2, kappa=None, lengthscale=None, jitter=1e-8):\n",
    "        \"\"\" compute and returns the NxM kernel matrix between the two sets of input X1 (shape NxD) and X2 (MxD) using the stationary and isotropic covariance function specified by self.kernel_fun\n",
    "    \n",
    "        arguments:\n",
    "            X1              -- NxD matrix\n",
    "            X2              -- MxD matrix\n",
    "            kappa           -- magnitude (positive scalar)\n",
    "            lengthscale     -- characteristic lengthscale (positive scalar)\n",
    "            jitter          -- non-negative scalar\n",
    "        \n",
    "        returns\n",
    "            K               -- NxM matrix    \n",
    "        \"\"\"\n",
    "\n",
    "        # extract dimensions \n",
    "        N, M = X1.shape[0], X2.shape[0]\n",
    "\n",
    "        # prep hyperparameters\n",
    "        kappa = self.kappa if kappa is None else kappa\n",
    "        lengthscale = self.lengthscale if lengthscale is None else lengthscale\n",
    "\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "        \n",
    "        assert K.shape == (N, M), f\"The shape of K appears wrong. Expected shape ({N}, {M}), but the actual shape was {K.shape}. Please check your code. \"\n",
    "        return K\n",
    "\n",
    "        \n",
    "# create an Nx1 vector of equidistant points in [-6, 6]\n",
    "N = 1000\n",
    "X = np.linspace(-6, 6, N)[:, None]\n",
    "\n",
    "# hyperparameters\n",
    "kappa = 1.\n",
    "scale = 1.\n",
    "\n",
    "# number of samples to be plotted\n",
    "num_samples = 10\n",
    "\n",
    "# instantiate kernel object and construct kernel\n",
    "kernel = StationaryIsotropicKernel(squared_exponential, kappa, scale)\n",
    "K = kernel.contruct_kernel(X, X)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "m = np.zeros(len(X))\n",
    "im = ax[0].pcolormesh(X.flatten(), X.flatten(), K, shading='auto')\n",
    "ax[0].set(xlabel='Input $x$', ylabel=\"Input $x'$\", title=f\"Kernel function $k(x, x')$ for $\\kappa = {kappa:2.1f}$ and $\\ell$ = {scale:2.1f}\")\n",
    "ax[0].grid(False)\n",
    "ax[0].set_aspect('equal')\n",
    "add_colorbar(im, fig, ax[0])\n",
    "\n",
    "f_samples = generate_samples(m, K, num_samples=num_samples, jitter=1e-8)\n",
    "ax[1].plot(X, f_samples, alpha=0.75, linewidth=3);\n",
    "ax[1].grid(True)\n",
    "ax[1].set(xlabel='$x$', ylabel='$f(x)$', title='Samples from the Gaussian process');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, the cell above you produce a plot of the covariance matrix as well as a plotting a number of samples form the corresponding Gaussian process.\n",
    "\n",
    "**Task 2.3**: Inspect the plots above. Change the parameters $\\kappa, \\ell$ and explain how they affect the structure of the kernel (left panel)  [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.4**: Inspect the plots above. Change the parameters $\\kappa, \\ell$ and explain how they affect the prior samples (right panel) [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5**: Generate samples from the Matern $\\frac12$ and Matern $\\frac{3}{2}$ kernels. How do they compare to samples from a squared exponential [**Discussion question**]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to understand and compare the three covariance functions (squared exponential, Matérn12, and Matérn32) is to study how they decay as a function of $||\\tau|| = ||\\mathbf{x}_n - \\mathbf{x}_m||$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = np.linspace(0, 8, 200)\n",
    "\n",
    "kappa = 1\n",
    "scale = 1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(tau, squared_exponential(tau, kappa=kappa, lengthscale=scale), linewidth=3, label='Squared exponential')\n",
    "ax.plot(tau, matern12(tau, kappa=kappa, lengthscale=scale), linewidth=3, label='Matern1/2')\n",
    "ax.plot(tau, matern32(tau, kappa=kappa, lengthscale=scale), linewidth=3, label='Matern3/2')\n",
    "ax.set(xlabel='$||\\\\tau||$', ylabel='$k(||\\\\tau||)$', title='Decay of stationary covariance functions')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the distance between two input points $\\mathbf{x}_n$ and $\\mathbf{x}_m$ is  $d\\cdot \\ell$, i.e. $\\|\\tau\\| = \\|\\mathbf{x}_n - \\mathbf{x}_m\\| = d\\cdot\\ell$.\n",
    "\n",
    "**Task 2.6**: Using the squared exponential covariance function and assume $\\kappa = 1$, what is the covariance between the function values $f(\\mathbf{x}_n)$ and $f(\\mathbf{x}_m)$ for $d = 0, 1, 2, 3, 4, 5$?\n",
    "\n",
    "*Hints: Use eq. (5)*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3:  Non-linear regression using Gaussian processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will study how Gaussian processes can be used for non-linear regression. First, we will study a simple toy data set $\\mathcal{D} = \\left\\lbrace x_n, y_n \\right\\rbrace_{n=1}^N$ for $N = 50$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = np.load('./data_exercise5.npz')\n",
    "N = data['N']\n",
    "Xtrain, ytrain = data['X'], data['y']\n",
    "\n",
    "# for predictions\n",
    "Xstar = np.linspace(-3, 9, 100)[:, None]\n",
    "\n",
    "# function for plotting\n",
    "def plot_data(ax):\n",
    "    ax.plot(Xtrain, ytrain, 'k.', markersize=12, label='Data')\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel('Input $x$')\n",
    "    ax.set_ylabel('Response $y$')\n",
    "    ax.legend()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "plot_data(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will adopt the following model\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_n = f(\\mathbf{x}_n) + e_n,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $f(\\mathbf{x})$ is assumed to be a Gaussian process. Assuming the noise $e_n$ is i.i.d. and Gaussian, the joint model for the training data becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{y}, \\mathbf{f}) = \\mathcal{N}\\left(\\mathbf{y}|\\mathbf{f}, \\sigma^2\\mathbf{I}\\right)\\mathcal{N}\\left(\\mathbf{f} | \\mathbf{0}, \\mathbf{K}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{K}_{nm} = k(\\mathbf{x}_n, \\mathbf{x}_m)$ is the covariance matrix. \n",
    "We will also use the squared exponential and Matérn covariance functions in this part. Thus, this model (when using these covariance functions) has three hyperparameters in total: $\\mathbf{\\theta} = \\left\\lbrace \\sigma, \\kappa, \\ell\\right\\rbrace$, where \n",
    "\n",
    "- $\\sigma > 0$ is the standard deviation of the noise, \n",
    "\n",
    "- $\\kappa > 0$ is the magnitude of the kernel,\n",
    "\n",
    "- $\\ell > 0$ is the lengthscale of the kernel. \n",
    "\n",
    "We will use $\\mathbf{\\theta}_K = \\left\\lbrace \\kappa, \\ell\\ \\right\\rbrace$ to denote the hyperparameters of the kernel $K$ and $\\mathbf{\\theta}$ to denote all hyperparameters of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you are given an incomplete implementation of a class for Gaussian process regression. The function called **compute_posterior** is supposed to the posterior mean and covariance matrix of the function values $f(\\mathbf{x}^*)$ for input $\\mathbf{x}^*$ conditioned on the training data $\\mathbf{y}$.\n",
    "The class contains a template for the following functions:\n",
    "\n",
    "- `predict_f`: computes the posterior distribution of the function value $f(\\mathbf{x}^*)$ for a new input, $\\mathbf{x}^*$, i.e. $p(f^*|\\mathbf{y}) = \\mathcal{N}(f^*|\\mu_{f^*}, \\Sigma_{f^*})$\n",
    "- `predict_y`: computes the posterior predictive distribution of for a new observation at new input, $\\mathbf{x}^*$, i.e. $p(y^*|\\mathbf{y}) = \\mathcal{N}(y^*|\\mu_{y^*}, \\Sigma_{y^*})$\n",
    "- `posterior_samples`: generates samples from the posterior distribution of $f^*$ for a set of inputs, i.e. $\\mathbf{f}^* \\sim p(f^*|\\mathbf{y})$\n",
    "- `log_marginal_likelihood`: evaluate the log marginal likelihood for a given set if hyperparameters $\\theta$\n",
    "\n",
    "In this part, we will focus on the first three and then come back to the marginal likelihood later.\n",
    "\n",
    "**Task 3.1**: Complete the implementation of the function `predict_f` below.\n",
    "\n",
    "*Hints*:\n",
    "- *Look at eq. (17.34), (17.35) and (17.36) in Murphy1*\n",
    "- *For computing matrix-vector prodcuts of the form $\\mathbf{A}^{-1} \\mathbf{x}$, `np.linalg.solve(A, x)` is numerically much more stable than `np.linalg.inv(A)@x`*.\n",
    "- *If you prefer, you can move the class to a separate python module and import it to the notebook if you prefer not the scroll up and down during as you make it through the notebook*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.2**: Complete the implementation of the function `predict_y` below.\n",
    "\n",
    "*Hints: Use the function `predict_f` you implemented in the previous task to do most of the work*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3**: Complete the implementation of the function `posterior_samples` below.\n",
    "\n",
    "*Hints: The implementation of this should be easy and make heavy use of functions you already implemented*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_uncertainty(ax, Xp, gp, color='r', color_samples='b', title=\"\", num_samples=0):\n",
    "    \n",
    "    mu, Sigma = gp.predict_y(Xp)\n",
    "    mean = mu.ravel()\n",
    "    std = np.sqrt(np.diag(Sigma))\n",
    "\n",
    "    # plot distribution\n",
    "    ax.plot(Xp, mean, color=color, label='Mean')\n",
    "    ax.plot(Xp, mean + 2*std, color=color, linestyle='--')\n",
    "    ax.plot(Xp, mean - 2*std, color=color, linestyle='--')\n",
    "    ax.fill_between(Xp.ravel(), mean - 2*std, mean + 2*std, color=color, alpha=0.25, label='95% interval')\n",
    "    \n",
    "    # generate samples\n",
    "    if num_samples > 0:\n",
    "        fs = gp.posterior_samples(Xstar, num_samples)\n",
    "        ax.plot(Xp, fs[:,0], color=color_samples, alpha=.25, label=\"$f(x)$ samples\")\n",
    "        ax.plot(Xp, fs[:, 1:], color=color_samples, alpha=.25)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class GaussianProcessRegression(object):\n",
    "\n",
    "    def __init__(self, X, y, kernel, scale=1., lengthscale=1., sigma=1/2, jitter=1e-8):\n",
    "        \"\"\"  \n",
    "        Arguments:\n",
    "            X                -- NxD input points\n",
    "            y                -- Nx1 observed values \n",
    "            kernel           -- must be instance of the StationaryIsotropicKernel class\n",
    "            jitter           -- non-negative scaler\n",
    "            kappa            -- magnitude (positive scalar)\n",
    "            lengthscale      -- characteristic lengthscale (positive scalar)\n",
    "            sigma            -- noise std. dev. (positive scalar)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.N = len(X)\n",
    "        self.kernel = kernel\n",
    "        self.jitter = jitter\n",
    "        self.set_hyperparameters(kappa, lengthscale, sigma)\n",
    "\n",
    "    def set_hyperparameters(self, kappa, lengthscale, sigma):\n",
    "        self.kappa = kappa\n",
    "        self.lengthscale = lengthscale\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def posterior_samples(self, Xstar, num_samples):\n",
    "        \"\"\"\n",
    "            generate samples from the posterior p(f^*|y, x^*) for each of the inputs in Xstar\n",
    "\n",
    "            Arguments:\n",
    "                Xstar            -- PxD prediction points\n",
    "        \n",
    "            returns:\n",
    "                f_samples        -- numpy array of (P, num_samples) containing num_samples for each of the P inputs in Xstar\n",
    "        \"\"\"\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "\n",
    "        assert (f_samples.shape == (len(Xstar), num_samples)), f\"The shape of the posterior mu seems wrong. Expected ({len(Xstar)}, {num_samples}), but actual shape was {f_samples.shape}. Please check implementation\"\n",
    "        return f_samples\n",
    "        \n",
    "    def predict_y(self, Xstar):\n",
    "        \"\"\" returns the posterior distribution of y^* evaluated at each of the points in x^* conditioned on (X, y)\n",
    "        \n",
    "        Arguments:\n",
    "        Xstar            -- PxD prediction points\n",
    "        \n",
    "        returns:\n",
    "        mu               -- Px1 mean vector\n",
    "        Sigma            -- PxP covariance matrix\n",
    "        \"\"\"\n",
    "\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "\n",
    "        return mu, Sigma\n",
    "\n",
    "    def predict_f(self, Xstar):\n",
    "        \"\"\" returns the posterior distribution of f^* evaluated at each of the points in x^* conditioned on (X, y)\n",
    "        \n",
    "        Arguments:\n",
    "        Xstar            -- PxD prediction points\n",
    "        \n",
    "        returns:\n",
    "        mu               -- Px1 mean vector\n",
    "        Sigma            -- PxP covariance matrix\n",
    "        \"\"\"\n",
    "\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "\n",
    "        # sanity check for dimensions\n",
    "        assert (mu.shape == (len(Xstar), 1)), f\"The shape of the posterior mu seems wrong. Expected ({len(Xstar)}, 1), but actual shape was {mu.shape}. Please check implementation\"\n",
    "        assert (Sigma.shape == (len(Xstar), len(Xstar))), f\"The shape of the posterior Sigma seems wrong. Expected ({len(Xstar)}, {len(Xstar)}), but actual shape was {Sigma.shape}. Please check implementation\"\n",
    "\n",
    "        return mu, Sigma\n",
    "    \n",
    "    def log_marginal_likelihood(self, kappa, lengthscale, sigma):\n",
    "        \"\"\" \n",
    "            evaluate the log marginal likelihood p(y) given the hyperparaemters \n",
    "\n",
    "            Arguments:\n",
    "            kappa       -- positive scalar \n",
    "            lengthscale -- positive scalar\n",
    "            sigma       -- positive scalar\n",
    "            \"\"\"\n",
    "\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "        \n",
    "\n",
    "# instantiate kernel\n",
    "kernel = StationaryIsotropicKernel(kernel_fun=squared_exponential)\n",
    "\n",
    "# instantiate GP without data (hence, posterior=prior) and with data\n",
    "gp_prior = GaussianProcessRegression(np.zeros((0, 1)), np.zeros((0, 1)), kernel)\n",
    "gp_post = GaussianProcessRegression(Xtrain, ytrain, kernel)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(25, 6))\n",
    "plot_data(ax[0])\n",
    "plot_with_uncertainty(ax[0], Xstar, gp_prior, title='Prior predictive distribution', num_samples=30)\n",
    "ax[0].legend(loc='lower center', ncol=4)\n",
    "ax[0].set_ylim((-5, 5))\n",
    "\n",
    "plot_data(ax[1])\n",
    "plot_with_uncertainty(ax[1], Xstar, gp_post, title='Posterior predictive distribution', num_samples=30)\n",
    "ax[1].legend(loc='lower center', ncol=4)\n",
    "ax[1].set_ylim((-5, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the code has been implemented correct, then the left figure above shows 30 samples from the prior predictive distribution along with the mean and 95% interval from the prior predictive distribution, whereas the right figure shows the same quantities for the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4**: Inspect the plots above. How does the prior and posterior predictive differ in: a) regions close to the data points? b) in regions far from the data point? [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5**: Explain this behavior using the equations for the posterior mean and covariance of $f^*$ (and what you saw in part 2)? [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.6**: Visualize the prior covariance matrix for $f^*$ and posterior covariance for $f^*$ matrix side-by-side as images and comment on what you see\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.7**: Change the values of kappa and scale and explain how it affects the prior and posterior. For example, what happens if you choose the lengthscale $\\ell$ to be either 0.25, 1, or 5 while keeping $\\kappa = 1$ and $\\sigma = \\frac12$? Do similar experiments with $\\sigma$ and $\\kappa$. [**Discussion question**]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 3.8**: Experiment with the two other kernerls, the Matérn $\\frac12$ and the Matérn $\\frac{3}{2}$. How does that affect the fits and the predictions? [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4:  Hyperparameter optimization using the marginal likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can apply this model in practice, we will study how we can use the ***marginal likelihood*** to estimate the hyperparameters of the model. If we let $\\mathbf{\\theta} \\in \\mathbb{R}^K$ denote all of our hyperparameters, then the **evidence approximation** suggests that we can estimate $\\mathbf{\\theta}$ as follows\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{\\theta}} = \\arg\\max\\limits_{\\mathbf{\\theta}} \\ln p(\\mathbf{y}|\\mathbf{\\theta}),\n",
    "\\end{align*}\n",
    "\n",
    "where $p(\\mathbf{y}|\\mathbf{\\theta})$ is the marginal likelihood of the model (see eq. (17.52) in Murphy1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.1**: Go back up to the implementation of `GaussianProcessRegression`-class and complete the implementation of the function `log_marginal_likelihood`\n",
    "\n",
    "- *Hints: The expression for the marginal likelihood contains both determinants and matrix inverse, which can be numerically fragile and lead to underflow/overflow. Hence, you need to be careful with the numerical implementation. See the details in the slides for this week*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, then the code below implement an optimization routine for the marginal likelihood. Note that we re-parametrize the hyperparameters in the log-domain before optimizing to ensure that the hyperparameter remain positive. We rely and the package `autograd` for computing the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(gp, theta_init):\n",
    "\n",
    "    # define optimization objective as the negative log marginal likelihood\n",
    "    objective = lambda params: -gp.log_marginal_likelihood(np.exp(params[0]), np.exp(params[1]), np.exp(params[2]))\n",
    "\n",
    "    # optimize using gradients\n",
    "    res = minimize(value_and_grad(objective), np.log(theta_init), jac=True)\n",
    "\n",
    "    # check for success\n",
    "    if not res.success:\n",
    "        print('Warning: optimization failed!')\n",
    "\n",
    "    # return resultss\n",
    "    theta = np.exp(res.x)\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it on our toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel = StationaryIsotropicKernel(kernel_fun=squared_exponential)\n",
    "gp_prior = GaussianProcessRegression(np.zeros((0, 1)), np.zeros((0, 1)), kernel)\n",
    "gp_post = GaussianProcessRegression(Xtrain, ytrain, kernel)\n",
    "\n",
    "# optimize\n",
    "theta_hat = optimize_hyperparameters(gp_post, theta_init=np.array([1,1,1]))\n",
    "\n",
    "# extract individual hyperparameters\n",
    "kappa_hat, scale_hat, sigma_hat = theta_hat\n",
    "\n",
    "# print\n",
    "print('Estimated hyperparameters')\n",
    "print(f'\\tsigma: {sigma_hat:3.2f}')\n",
    "print(f'\\tkappa: {kappa_hat:3.2f}')\n",
    "print(f'\\tscale: {scale_hat:3.2f}')\n",
    "\n",
    "\n",
    "gp_prior = GaussianProcessRegression(np.zeros((0, 1)), np.zeros((0, 1)), kernel)\n",
    "gp_post = GaussianProcessRegression(Xtrain, ytrain, kernel)\n",
    "gp_prior.set_hyperparameters(kappa_hat, scale_hat, sigma_hat)\n",
    "gp_post.set_hyperparameters(kappa_hat, scale_hat, sigma_hat)\n",
    "\n",
    "mu_prior, Sigma_prior = gp_prior.predict_f(Xstar)\n",
    "mu_post, Sigma_post = gp_post.predict_f(Xstar)\n",
    "    \n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "plot_data(ax[0])\n",
    "plot_with_uncertainty(ax[0], Xstar, gp_prior, title='Prior predictive distribution', num_samples=30)\n",
    "ax[0].legend(loc='lower center', ncol=3)\n",
    "ax[0].set_ylim((-9, 9))\n",
    "plot_data(ax[1])\n",
    "plot_with_uncertainty(ax[1], Xstar, gp_post, title='Posterior predictive distribution', num_samples=30)\n",
    "ax[1].legend(loc='lower center', ncol=3)\n",
    "ax[1].set_ylim((-9, 9));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.2**: Comment on the quality of the fit. Does it seem reasonable? [**Discussion question**]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.3**: If we were to fit the three hyperparameters using 5-fold cross-validation and a grid search rather than using the marginal likelihood, how many times would we have to train the model? Assume we would 10 different values for each hyperparameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5:  Analysing the bike sharing data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, your task is to fit a GP regression model (with squared exponential kernel) to a subset of the bike sharing dataset (Source: [https://ride.capitalbikeshare.com/system-data](https://ride.capitalbikeshare.com/system-data)). The dataset consists of the number of rented bikes in 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.load('./data_exercise5b.npz')\n",
    "day = data['day']\n",
    "bike_count = data['bike_count']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(day, bike_count, 'k.', label='Data')\n",
    "ax.legend()\n",
    "ax.set(xlabel='Days since 1/1-2011', ylabel='Bike count');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.1**: Log-transform the bike counts, remove the mean and scale to unit variance. Plot the preprocessed data set.\n",
    "\n",
    "\n",
    "\n",
    "**Task 5.2**: Estimate the hyperparameters of a Gaussian process regression model with a squared exponential kernel from the preprocessed data. Report the estimated values of each hyperparameter.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5691287c844ba9dcbbcdccebf31ad10ecb418867ae27f8b1f19557c789afbaf7"
  },
  "kernelspec": {
   "display_name": "bml2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
