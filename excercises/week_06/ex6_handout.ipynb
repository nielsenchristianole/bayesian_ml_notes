{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import autograd.numpy as np\n",
    "import pylab as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as snb\n",
    "\n",
    "from exercise6 import compute_err\n",
    "from exercise6 import eval_density_grid\n",
    "from exercise6 import load_MNIST_subset\n",
    "from exercise6 import plot_with_uncertainty\n",
    "from exercise6 import add_colorbar\n",
    "from exercise6 import StationaryIsotropicKernel\n",
    "from exercise6 import squared_exponential\n",
    "from exercise6 import matern12\n",
    "from exercise6 import matern32\n",
    "from exercise6 import NeuralNetworkMAP\n",
    "from exercise6 import generate_samples\n",
    "\n",
    "# from autograd import grad, \n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import binom as binom_dist\n",
    "\n",
    "snb.set_style('darkgrid')\n",
    "snb.set_theme(font_scale=1.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02477 Bayesian Machine Learning: Gaussian processes classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to\n",
    "- get familiar with Bayesian modelling with non-Gaussian likelihoods, we will study the Bernoulli likelihood in particular.\n",
    "- study Gaussian processes for binary classification via the Laplace approximation\n",
    "- dive deeper into Gaussian processes\n",
    "\n",
    "\n",
    "**Content**\n",
    "\n",
    "- Part 1: Theory for binary classification using Gaussian process\n",
    "- Part 2: Implementing the Laplace approximation (Step 1 of 3)\n",
    "- Part 3: Implementing the posterior distribution of $p(f^*|\\mathbf{y}, \\mathbf{x}^*)$ (step 2 of 3)\n",
    "- Part 4: Compute the posterior predictive distribution of the label $y^*$ for the input point $\\mathbf{x}^*$, i.e. to compute $p(y^*|\\mathbf{y}, \\mathbf{x}^*)$. (Step 3 of 3)\n",
    "- Part 5: Gaussian process classification for 2D data set\n",
    "\n",
    "\n",
    "**Note**: In this exercise, we will use the code from last week for the squared exponential kernel etc. We will also make use of a set of auxiliary functions, which are less important for the learning objectives of today and therefore, they can found in the module `exercise6.py`.\n",
    "\n",
    "**Note**: The exercise contains several **discussion questions**, which are questions, where are supposed to actively experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs. Instead of proving the full description for every discussion question, we simply tag it with: [**Discussion question**] after the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will implement and test everything on a simple 1D toy dataset. The dataset is generated using the following model:\n",
    "\n",
    "$$\\begin{align*}\n",
    "y_n|x_n &\\sim \\text{Ber}(\\sigma(f(x_n)))\\\\\n",
    "f(x_n) &= 5 \\sin\\left(\\frac{3}{4}x\\right)\\\\\n",
    "x_n &\\sim \\mathcal{N}(0, 2.5^2),\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence, the true data generating process is $p(y=1|x) = \\sigma(5\\sin(\\frac{3}{4} x))$, where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the logistic sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "sigmoid = lambda x: 1./(1+np.exp(-x))\n",
    "\n",
    "# true p(y=1|x) = sigmoid(f(x))\n",
    "f = lambda x:  5*np.sin(0.75*x)\n",
    "\n",
    "N = 50\n",
    "X = np.sort(np.random.normal(0, 2.5, size=N))[:, None]\n",
    "y = np.random.binomial(1, sigmoid(f(X)))\n",
    "\n",
    "# define points for prediction/plotting\n",
    "Xstar = np.linspace(-7.5, 7.5, 300)[:, None]\n",
    "\n",
    "# plot data\n",
    "def plot_data(ax, X, y, title=\"Synthetic data\", plot_true=False):\n",
    "    ax.plot(X[y==0], y[y==0], 'bo', label='Class 0')\n",
    "    ax.plot(X[y==1], y[y==1], 'ro', label='Class 1')\n",
    "    if plot_true:\n",
    "        ax.plot(Xstar, sigmoid(f(Xstar)), 'g--', label='True p(y=1|x)')\n",
    "    ax.set(xlabel='Input feature $x$', ylabel='Target $y$', title=title)\n",
    "    ax.legend()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 4))\n",
    "plot_data(ax, X, y, plot_true=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:  Theory for binary classification using Gaussian process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will study Gaussian processes for binary classification using the following model\n",
    "\n",
    "$$\\begin{align*}\n",
    "y|f(\\mathbf{x}) &\\sim \\text{Ber}[\\sigma(f(\\mathbf{x}))]\\\\\n",
    "f(\\mathbf{x}) &\\sim \\mathcal{GP}(0, k(\\mathbf{x}, \\mathbf{x}')),\n",
    "\\end{align*}$$\n",
    "\n",
    "where $k(\\mathbf{x}, \\mathbf{x}')$ is the covariance function and $\\sigma: \\mathbb{R} \\rightarrow (0, 1)$ is a suitable inverse link function. Here we will use the logistic sigmoid function. The purpose of the function $\\sigma$ is to \"squeeze\" the values of $y(\\mathbf{x})$ from the entire real line to the unit interval such that we can interpret $\\sigma(f(\\mathbf{x})) \\in \\left[0, 1\\right]$ as a probability for all $\\mathbf{x}$.\n",
    "\n",
    "Our goal is to compute the predictive distribution $p(y^* = 1|\\mathbf{y}, \\mathbf{x}^*)$ for the class label $y^*$ of a new input point $\\mathbf{x}^*$ given a dataset $\\mathcal{D} = \\left\\lbrace \\mathbf{x}_n, y_n \\right\\rbrace_{n=1}^N$ for $\\mathbf{x} \\in \\mathbb{R}^D$ and $y_n \\in \\left\\lbrace 0, 1 \\right\\rbrace$. Let $\\mathbf{y} \\in \\left\\lbrace 0, 1 \\right\\rbrace^N$ be the vector of binary observations and let $\\mathbf{f} = \\begin{bmatrix}f(\\mathbf{x}_1) & f(\\mathbf{x}_2) & \\dots & f(\\mathbf{x}_N) \\end{bmatrix} \\in \\mathbb{R}^N$ be the corresponding vector of latent function values for each sample in the training set, then the joint model of the observations $\\mathbf{y}$ and the latent function values $\\mathbf{f}$ becomes\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\mathbf{y}, \\mathbf{f}) = p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f}) = \\prod_{n=1}^N \\text{Ber}(y_n|\\sigma(f_n)) \\, \\mathcal{N}(\\mathbf{f}|\\mathbf{0}, \\mathbf{K}),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where we have assumed the observation to be conditionally independent given $\\mathbf{f}$, and $\\mathbf{K} \\in \\mathbf{R}^{N \\times N}$ is the prior covariance matrix of $\\mathbf{f}$, i.e. $\\mathbf{K}_{nm} = k(\\mathbf{x}_n, \\mathbf{x}_m)$.\n",
    "\n",
    "#### A 3-step approach for computing the posterior predictive distribution for $y^*$\n",
    "\n",
    "Just like for the Bayesian logistic regression model, exact analytical Bayesian inference is intractable for this model, and therefore, we will resort to the Laplace approximation. Making predictions for a Gaussian process classification model using the Laplace approximation typically require threes steps:\n",
    "\n",
    "- Step 1: Aproximating the posterior distribution $p(\\mathbf{f}|\\mathbf{y})$\n",
    "\n",
    "- Step 2: Computing the posterior distribution of the latent function evaluated at a new point $\\mathbf{x}^*$, i.e computing $p(f^*|\\mathbf{y}, \\mathbf{x}^*)$ where $f^* = f(\\mathbf{x}^*)$.\n",
    "\n",
    "- Step 3: Computing the posterior predictive distribution of the label $y^*$ for the input point $\\mathbf{x}^*$, i.e. computing $p(y^*|\\mathbf{y}, \\mathbf{x}^*)$.\n",
    "\n",
    "**Note**: For the regression models with Gaussian likelihoods, there is often a lot of confusion about the difference between $f^*$ and $y^*$. Part of this confusion is often caused by the fact that for regression $f^*$ and $y^*$ typically belong to the same mathematical space, e.g. $f^*, y^* \\in \\mathbf{R}$ and often $f^*$ can be interpreted as a *denoised* version of $y^*$. However, for binary classification models, the two quantities belong to different spaces, i.e. $f^* \\in \\mathbb{R}$ and $y^* \\in \\left\\lbrace 0, 1 \\right\\rbrace$ and here it might be more clear that $f(\\mathbf{x})$ simply is a latent function, which we introduce in order to model the distribution $\\text{Ber}(y^*|\\sigma(f^*))$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:  Implementing the Laplace approximation (Step 1 of 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is now to implement the Laplace approximation for $p(\\mathbf{f}|\\mathbf{y})$ such that\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\mathbf{f}|\\mathbf{y}) = \\frac{p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})}{p(\\mathbf{y})}\\approx q(\\mathbf{f}) = \\mathcal{N}(\\mathbf{f}|\\mathbf{m}, \\mathbf{S}),\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\mathbf{m} = \\mathbf{f}_{\\text{MAP}}$ is the MAP estimator for $\\mathbf{f}$ and $\\mathbf{S}$ is the negative inverse Hessian. As usual, we set-up the log joint distribution\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\log p(\\mathbf{y}, \\mathbf{f}) &= \\log p(\\mathbf{y}|\\mathbf{f}) + \\log p(\\mathbf{f})\n",
    "%\n",
    "= \\sum_{n=1}^N \\log  p(y_n|f_n) - \\frac{N}{2}\\log (2\\pi) - \\frac12 | \\mathbf{K}| - \\frac12 \\mathbf{f}^T\\mathbf{K}^{-1} \\mathbf{f} \n",
    "\\end{align*}$$\n",
    "\n",
    "where the gradient and the Hessian are given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla_{\\mathbf{f}} \\log p(\\mathbf{y}, \\mathbf{f}) &= \\sum_{n=1}^N \\nabla_{\\mathbf{f}}\\log  p(y_n|f_n) - \\frac12 \\nabla_{\\mathbf{f}}\\mathbf{f}^T\\mathbf{K}^{-1} \\mathbf{f} = \\mathbf{g} - \\mathbf{K}^{-1} \\mathbf{f},\\\\\n",
    "%\n",
    "\\nabla_{\\mathbf{f}}^2 \\log p(\\mathbf{y}, \\mathbf{f}) &= \\sum_{n=1}^N  \\nabla_{\\mathbf{f}}^2 \\log  p(y_n|f_n) - \\frac12 \\nabla_{\\mathbf{f}}^2\\mathbf{f}^T\\mathbf{K}^{-1} \\mathbf{f}\n",
    "=\\sum_{n=1}^N  \\nabla_{\\mathbf{f}}^2 \\log  p(y_n|f_n) - \\mathbf{K}^{-1} \n",
    "=-\\Lambda - \\mathbf{K}^{-1},\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\mathbf{g} \\in \\mathbb{R}^N$ with entries given by $g_n = \\frac{\\partial}{\\partial f_n}\\log  p(y_n|f_n)$ and $\\Lambda \\in \\mathbb{R}^{N \\times N}$ is a diagonal matrix with elements $\\Lambda_{nn} = -\\nabla_{\\mathbf{f}}^2 \\log  p(y_n|f_n)$ evaluated at the mode $\\hat{\\mathbf{f}}_{\\text{MAP}} = \\argmax_{\\mathbf{f}} \\log p(\\mathbf{y}, \\mathbf{f})$. Thus, the mean and covariance matrix for the Laplace approximation $q(\\mathbf{f}) = \\mathcal{N}(\\mathbf{f}|\\mathbf{m}, \\mathbf{S})$ become \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{m} &= \\hat{\\mathbf{f}}_{\\text{MAP}}\\\\\n",
    "\\mathbf{S} &= \\left(\\mathbf{K}^{-1} + \\Lambda\\right)^{-1}\n",
    "\\end{align*}$$\n",
    "\n",
    "The expression for the posterior covariance matrix $\\mathbf{S}$ involves the inverse of $\\mathbf{K}$, which we know can be numerically unstable. To avoid having to compute the inverse of $\\mathbf{K}$ explicitly, we will use the [**Woodbury matrix identity**](https://en.wikipedia.org/wiki/Woodbury_matrix_identity) to re-write the expression for the posterior covariance as \n",
    "$$\\begin{align*}\n",
    "\\mathbf{S} &= \\left(\\mathbf{K}^{-1} + \\Lambda\\right)^{-1} = \\mathbf{K} - \\mathbf{K}\\Lambda^{\\frac12} (\\underbrace{\\mathbf{I} + \\Lambda^{\\frac12}\\mathbf{K}\\Lambda^{\\frac12}}_{\\mathbf{B}})^{-1} \\Lambda^{\\frac12} \\mathbf{K}\n",
    "\\end{align*}$$\n",
    "\n",
    "This might seem like a complicated way of calculating something simple, but the key thing is that the matrix $\\mathbf{B} = \\mathbf{I} + \\Lambda^{\\frac12}\\mathbf{K}\\Lambda^{\\frac12}$ is usually well-behaved because its eigenvalues are lowerbounded by $1$, and hence, the linear system $\\mathbf{B}^{-1}\\Lambda^{\\frac12} \\mathbf{K}$ can usually be solved without problems, e.g. via Cholesky decompositions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The contributions of the likelihood to the gradient and Hessian\n",
    "\n",
    "By inspecting the equations above, we see that both the gradient and Hessian contains a contribution from the prior and a contribution from the likelihood. First, will focus on implementing the contributions from the likelihood. We need to compute the first and second order derivatives of the log likelihood $\\log p(y_n|f_n) = \\log \\text{Ber}(y_n|\\sigma(f_n))$\n",
    "\n",
    "**Task 2.1**: Show that first and second order derivative of $\\log p(y_n|f_n)$ is given by $\\frac{\\partial}{\\partial f_n}\\log p(y_n|f_n) = y_n -\\sigma(f_n)$ and $\\frac{\\partial^2}{\\partial f_n^2}\\log p(y_n|f_n) = - \\sigma(f_n)(1-\\sigma(f_n)$.\n",
    "\n",
    "*Hints: Recall the identities $\\frac{\\partial}{\\partial f} \\sigma(f) = \\sigma(f)(1-\\sigma(f))$ and $\\frac{\\partial}{\\partial x} \\log (x) = \\frac{1}{x}$*. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement the contribution from the likelihood in the class `BernoulliLikelihood` below.\n",
    "\n",
    "**Task 2.2**: Complete the implementation of the functions `log_lik`, `grad`, and `hessian` of the class below.\n",
    "\n",
    "*Hints: Each function should be fairly straight forward and consist of 1-2 lines of code each. Regarding the likelihood, you can either implement it from scratch or use the relevant module from scipy.stats*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliLikelihood(object):\n",
    "    \"\"\" Implement the Bernoulli likelihood with the sigmoid as inverse link function \"\"\"\n",
    "\n",
    "    def __init__(self, y):\n",
    "        # store data & force shape (N, )\n",
    "        self.y = y.ravel()\n",
    "\n",
    "    def log_lik(self, f):\n",
    "        \"\"\" Implements log p(y|f) = sum log p(y_n|f_n), where p(y_n|f_n) = Ber(y_n|sigmoid(f_n)). \n",
    "            \n",
    "            Argument:\n",
    "            f       --       vector of function values, shape (N, )\n",
    "\n",
    "            Returns\n",
    "            ll      --       sum of log likelihoods for all N data points, scalar\n",
    "\n",
    "        \"\"\"\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "\n",
    "        # check shape and return\n",
    "        assert ll.shape == (), f\"Expected shape for loglik_ is (), but the actual shape was {ll.shape}. Please check implementation\"\n",
    "        return ll\n",
    "    \n",
    "    def grad(self, f):\n",
    "        \"\"\" Implements the gradient of log p(y|n) \n",
    "\n",
    "            Argument:\n",
    "            f       --       vector of function values, shape (N, )\n",
    "\n",
    "            Returns\n",
    "            g       --       gradient of log p(y|f), i.e. a vector of first order derivatives with shape (N, )\n",
    "             \n",
    "        \"\"\"\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "        # check shape and return\n",
    "        assert g.shape == (len(f), ), f\"Expected shape for g is ({len(f)}, ), but the actual shape was {g.shape}. Please check implementation\"\n",
    "        return g\n",
    "\n",
    "    def hessian(self, f):\n",
    "        \"\"\" Implements the Hessian of log p(y|n) \n",
    "\n",
    "        Argument:\n",
    "            f       --       vector of function values, shape (N, )\n",
    "\n",
    "        Returns:\n",
    "            Lambda  --       Hessian of likelihood, i.e. a diagonal matrix with the second order derivatives on the diagonal, shape (N, N)\n",
    "        \"\"\"\n",
    "\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "\n",
    "        # check shape and return\n",
    "        assert Lambda.shape == (len(f), len(f)), f\"Expected shape for Lambda is ({len(f)}, {len(f)}), but the actual shape was {Lambda.shape}. Please check implementation\"\n",
    "        return Lambda\n",
    "\n",
    "\n",
    "# sanity check of implementation\n",
    "likelihood = BernoulliLikelihood(np.array([0, 1, 0, 0]))\n",
    "assert np.allclose(likelihood.log_lik(-np.ones((4))), -2.2530467500728912), \"The implementation of the function log_lik seems to be wrong. Please check your implementation (chech that the dimensionen match your expectations before summing over the data points)\"\n",
    "assert np.allclose(np.linalg.norm(likelihood.grad(np.ones((4)))), 1.2944765058872572), \"The implementation of the function grad seems to be wrong. Please check your implementation\"\n",
    "assert np.allclose(np.linalg.norm(likelihood.hessian(np.ones((4)))), 0.3932238664829637), \"The implementation of the function hessian seems to be wrong. Please check your implementation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the implementation of the likelihood, we are ready to implement the Laplace approximation of the Gaussian process.\n",
    "\n",
    "We will compute $\\hat{\\mathbf{f}}_{\\text{MAP}}$ using the gradient-based optimization. To make the optimization problem easier and avoid solving the sensitive linear system $\\mathbf{K}^{-1} \\mathbf{f}$ in every iteration, we make use of the following reparametrization $\\mathbf{f} = \\mathbf{K}\\mathbf{a}$ when locating the mode. That is, we let $\\mathbf{f} = \\mathbf{K}\\mathbf{a}$ and optimize the log joint wrt. $\\mathbf{a} \\in \\mathbb{R}^N$ and then when the optimization terminates, we compute $\\mathbf{f}$ from $\\mathbf{a}$. \n",
    "\n",
    "Below you are given an incomplete implementation of the class `GaussianProcessClassification`, which uses the `BernoulliLikelihood` we implemented above.\n",
    "\n",
    "**Task 2.3**: Study the class `GaussianProcessClassification` and complete the implementation of the function `construct_laplace_approximation`. The function already computes $\\mathbf{f}_{\\text{MAP}}$, so your task is to implement the Hessian.\n",
    "\n",
    "*Hints: Start by implementing the \"straightforward\" expression for $\\mathbf{S}$ and then move on to the more robust version if you have time. The straight-forward implementation should be sufficient for reasonable hyperparameters, but might be problematic for some hyperparameter values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probit = lambda x: norm.cdf(x)\n",
    "\n",
    "class GaussianProcessClassification(object):\n",
    "\n",
    "    def __init__(self, X, y, likelihood, kernel, kappa=1., lengthscale=1.,jitter=1e-8):\n",
    "        \"\"\"  \n",
    "        Arguments:\n",
    "            X                -- NxD input points\n",
    "            y                -- Nx1 observed values \n",
    "            likelihood       -- likelihood instance\n",
    "            kernel           -- must be instance of the StationaryIsotropicKernel class\n",
    "            jitter           -- non-negative scaler\n",
    "            kappa            -- magnitude (positive scalar)\n",
    "            lengthscale      -- characteristic lengthscale (positive scalar)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.N = len(X)\n",
    "        self.likelihood = likelihood(y)\n",
    "        self.kernel = kernel\n",
    "        self.jitter = jitter\n",
    "        self.set_hyperparameters(kappa, lengthscale)\n",
    "\n",
    "        # precompute kernel, its Cholesky decomposition and prepare Laplace approx\n",
    "        self.K = self.kernel.contruct_kernel(self.X, self.X, jitter=self.jitter)\n",
    "        self.L = np.linalg.cholesky(self.K)\n",
    "        self.construct_laplace_approximation()\n",
    "\n",
    "    def set_hyperparameters(self, kappa, lengthscale):\n",
    "        self.kernel.kappa = kappa\n",
    "        self.kernel.lengthscale = lengthscale\n",
    "        \n",
    "    def log_joint_a(self, a):\n",
    "        \"\"\" computes and returns the log joint distribution log p(y, f), where f = K*a \"\"\"\n",
    "        f = self.K@a\n",
    "        # compute log prior contribution\n",
    "        const = -self.N/2*np.log(2*np.pi)\n",
    "        logdet = np.sum(np.log(np.diag(self.L)))\n",
    "        quad_term =  0.5*np.sum(a*f)\n",
    "        log_prior = const - logdet - quad_term\n",
    "        # compute log likelihood contribution\n",
    "        log_lik = self.likelihood.log_lik(f)\n",
    "        # return sum\n",
    "        return log_prior + log_lik\n",
    "    \n",
    "\n",
    "    def grad_a(self, a):\n",
    "        \"\"\" computes gradient of log joint distribution, i.e. log p(y, a) = log p(y|a) + log p(a), wrt. a \"\"\"\n",
    "        f = self.K@a\n",
    "        # compute gradient contribution from prior and likelihood\n",
    "        grad_prior = -f\n",
    "        grad_lik = self.likelihood.grad(f)@self.K\n",
    "        # sum and return\n",
    "        return grad_prior + grad_lik\n",
    "        \n",
    "    \n",
    "    def compute_f_MAP(self):\n",
    "        # optimize to get f_MAP\n",
    "        result = minimize(lambda a: -self.log_joint_a(a), jac=lambda a: -self.grad_a(a), x0=np.zeros((self.N)))\n",
    "        \n",
    "        if not result.success:\n",
    "            print(result)\n",
    "            raise ValueError('Optization failed')\n",
    "        \n",
    "        self.a = result.x\n",
    "        f_MAP = self.K @ result.x\n",
    "        return f_MAP\n",
    "\n",
    "    def construct_laplace_approximation(self):\n",
    "\n",
    "        # f_MAP\n",
    "        self.m = self.compute_f_MAP()\n",
    "\n",
    "        # Compute Hessian\n",
    "        self.S = <insert code here>\n",
    "\n",
    "    def predict_f(self, Xstar):\n",
    "        \"\"\" returns the posterior distribution of f^* evaluated at each of the points in x^* conditioned on (X, y)\n",
    "        \n",
    "        Arguments:\n",
    "        Xstar            -- PxD prediction points\n",
    "        \n",
    "        returns:\n",
    "        mu               -- mean vector, shape (P,)\n",
    "        Sigma            -- covariance matrix, shape (P, P) \n",
    "        \"\"\"\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "\n",
    "        # check dimensions and return\n",
    "        assert (mu.shape == (len(Xstar),)), f\"Expected shape for mu is ({len(Xstar)}), but the actual shape was {mu.shape}. Please check implementation\"\n",
    "        assert Sigma.shape == (len(Xstar), len(Xstar)), f\"Expected shape for Sigma is ({len(Xstar)}, {len(Xstar)}), but the actual shape was {Sigma.shape}. Please check implementation\"\n",
    "\n",
    "        return mu, Sigma\n",
    "    \n",
    "    def predict_y(self, Xstar):\n",
    "        \"\"\" returns the posterior distribution of y^* evaluated at each of the points in x^* conditioned on (X, y)\n",
    "        \n",
    "        Arguments:\n",
    "        Xstar            -- PxD prediction points\n",
    "        \n",
    "        returns:\n",
    "        p               -- vector of post. pred. probabilities, shape (P,)\n",
    "        \"\"\"\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "\n",
    "        # check dimensions and return\n",
    "        assert (p.shape == (len(Xstar),)), f\"Expected shape for p is ({len(Xstar)}), but the actual shape was {p.shape}. Please check implementation\"\n",
    "        return p\n",
    "    \n",
    "    def posterior_samples(self, Xstar, num_samples):\n",
    "        \"\"\"\n",
    "            generate samples from the posterior p(f^*|y, x^*) for each of the inputs in Xstar\n",
    "\n",
    "            Arguments:\n",
    "                Xstar            -- PxD prediction points\n",
    "        \n",
    "            returns:\n",
    "                f_samples        -- numpy array of (P, num_samples) containing num_samples for each of the P inputs in Xstar\n",
    "        \"\"\"\n",
    "        mu, Sigma = self.predict_f(Xstar)\n",
    "        f_samples = generate_samples(mu.ravel(), Sigma, num_samples)\n",
    "\n",
    "        assert (f_samples.shape == (len(Xstar), num_samples)), f\"The shape of the posterior mu seems wrong. Expected ({len(Xstar)}, {num_samples}), but actual shape was {f_samples.shape}. Please check implementation\"\n",
    "        return f_samples\n",
    "\n",
    "# prep kernel, likelihood and Gaussian process\n",
    "kernel = StationaryIsotropicKernel(squared_exponential)\n",
    "likelihood = BernoulliLikelihood\n",
    "gpc = GaussianProcessClassification(X, y, likelihood, kernel, kappa=3., lengthscale=1)\n",
    "\n",
    "# plot data and visualize posterior\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "plot_data(ax, X, y)\n",
    "ax.plot(X, gpc.m, 'k.')\n",
    "ax.errorbar(X, gpc.m, yerr=1.96*np.sqrt(np.diag(gpc.S)), fmt='ko', label='Posterior mean and 95% interval for $f_n$', alpha=0.5)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, the code above visualizes the Laplace approximation superimposed on the data.\n",
    "\n",
    "**Task 2.4**: Explain what you see in the plot above. That is, what does the entries of the posterior mean vector $\\mathbf{m}$ represent? What about the entries in the posterior covariance matric $\\mathbf{S}$? [**Discussion question**]\n",
    "\n",
    "*Hints: What are the dimensions of the mean $\\mathbf{m}$ and the covariance $\\mathbf{S}$ of the Laplace approximation?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Implementing the posterior distribution of $p(f^*|\\mathbf{y}, \\mathbf{x}^*)$ (step 2 of 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready for step 2, which is to compute the posterior distribution of the latent function evaluated at a new point $\\mathbf{x}^*$, i.e to compute $p(f^*|\\mathbf{y}, \\mathbf{x}^*)$ where $f^* = f(\\mathbf{x}^*)$. Informally, we want to be able to \"fill the gaps\" in the plot above.\n",
    "\n",
    "Using the posterior approximation $q(\\mathbf{f}) = \\mathcal{N}(\\mathbf{f}|\\mathbf{m}, \\mathbf{S})$, we can compute the approximate posterior for $f^* = f(\\mathbf{x}^*)$ when evaluated at any $\\mathbf{x}^*$ as follows:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(f^*|\\mathbf{y}, \\mathbf{x}^*) &= \\int p(f^*|\\mathbf{f}, \\mathbf{x}^*) p(\\mathbf{f}|\\mathbf{y}) \\text{d} \\mathbf{f}\\approx  \\int p(f^*|\\mathbf{f}) q(\\mathbf{f}) \\text{d} \\mathbf{f}.\n",
    "\\end{align*}$$\n",
    "\n",
    "Since $p(\\mathbf{f}, f^*)$ is a multivariate Gaussian, the conditional distribution $p(f^*|\\mathbf{f})$ will also be a Gaussian, where the mean and covariance is given by the general equations on p. 84 in Murphy1 (you will derive them below). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Moreover, since $q(\\mathbf{f})$ is also Gaussian, we can solve the integral above by recognizing that this integral is equivalent to a marginalization in a linear Gaussian system (see Section 3.3 in Murphy1). The results becomes\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(f^*|\\mathbf{y}, \\mathbf{x}^*) \\approx  \\mathcal{N}(f^*|\\mu_{f^*|\\mathbf{y}}, \\sigma^2_{f^*|\\mathbf{y}}),\n",
    "\\end{align*}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mu_{f^*|\\mathbf{y}} &=  \\mathbf{k}^T \\mathbf{K}^{-1} \\mathbf{m},\\\\\n",
    "\\sigma^2_{f^*|\\mathbf{y}} &= c - \\mathbf{k}^T \\mathbf{K}^{-1} (\\mathbf{K} - \\mathbf{S}) \\mathbf{K}^{-1} \\mathbf{k}\n",
    "\\end{align*}$$\n",
    "\n",
    "where $c = k(\\mathbf{x}_*, \\mathbf{x}_*)$ is the prior variance of $f^*$, $\\mathbf{k} \\in \\mathbb{R}^N$ is a vector with entries given by $\\mathbf{k}_n = k(\\mathbf{x}_*, \\mathbf{x}_n)$, and $\\mathbf{K} \\in \\mathbb{R}^{N \\times N}$ is prior covariance of $\\mathbf{f}$. If you want to compute the joint distribution the predictions for $P$ input points $\\mathbf{x}^*$ at once, then $\\mathbf{k}$ becomes a $P \\times N$-matrix and $c$ becomes a $P \\times P$-matrix.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before implementing this part, we will elaborate a bit on some of the details above. \n",
    "\n",
    "**Task 3.1**: Write the expression for the prior mean and the prior covariance matrix for the joint Gaussian distribution $p( f^*, \\mathbf{f})$ in terms of the kernel quantitites $\\mathbf{K}$, $\\mathbf{k}$ and $k$.\n",
    "\n",
    "*Hints: What is the prior mean of $\\mathbf{f}$ and $f^*$? What is the prior variance of $\\mathbf{f}$ and $f^*$? What is the prior covariance between $\\mathbf{f}$ and $f^*$?*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The next task will make you of your solution from the previous task, so you might want to verify it before continuing.\n",
    "\n",
    "\n",
    "\n",
    "**Task 3.2**: Write the expression for the mean and covariance of the conditional distribution $p(f^*|\\mathbf{f}, \\mathbf{x}^*)$ using the solution to the previous task.\n",
    "\n",
    "*Hints: Use the equations (3.26) and (3.28)  on p. 84 in Murphy1.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to implemenet the distribution $p(f^*|\\mathbf{y}, \\mathbf{x}^*)$.\n",
    "\n",
    "**Task 3.3**: Go back up the class `GaussianProcessClassification` and complete the implementation of the function `predict_f` using the equations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep model\n",
    "kernel = StationaryIsotropicKernel(squared_exponential)\n",
    "likelihood = BernoulliLikelihood\n",
    "gpc = GaussianProcessClassification(X, y, likelihood, kernel, kappa=3, lengthscale=1)\n",
    "\n",
    "# predict\n",
    "mu, Sigma = gpc.predict_f(Xstar)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 4))\n",
    "plot_data(ax, X, y)\n",
    "ax.plot(X, gpc.m, 'k.')\n",
    "ax.errorbar(X, gpc.m, yerr=1.96*np.sqrt(np.diag(gpc.S)), fmt='ko', label='Posterior mean and 95% interval', alpha=0.2)\n",
    "plot_with_uncertainty(ax, Xstar, mu, Sigma, color='g', title='Posterior distribution $p(f^*|\\mathbf{y}, x^*)$', num_samples=10)\n",
    "ax.legend(ncol=5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4**: Play around with the kernel and hyperparameter settings and describe how they affect the resulting plot [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4:  Compute the posterior predictive distribution of the label $y^*$ for the input point $\\mathbf{x}^*$, i.e. to compute $p(y^*|\\mathbf{y}, \\mathbf{x}^*)$. (Step 3 of 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after having obtained the distribution $p(f^*|\\mathbf{y})$, we can use this distribution to compute the posterior predictive distribution $p(y^*|\\mathbf{y}, \\mathbf{x}^*)$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(y^* = 1|\\mathbf{y}, \\mathbf{x}^*) \\approx \\int \\sigma(f^*) p(f^*|\\mathbf{y}) \\text{d} f^*.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We will use the **probit approximation**, which approximates the sigmoid as follows: $\\sigma(f^*) \\approx \\Phi(f^* \\sqrt{\\frac{\\pi}{8}})$, where $\\Phi$ is the cumulative distribution function of the standard normal distribution. The benefit is that the expectation value of the approximation can be computed analytically as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(y^* = 1|\\mathbf{y}, \\mathbf{x}^*) \\approx \\int \\sigma(f^*) p(f^*|\\mathbf{y}, \\mathbf{x}^*) \\text{d} f^* \\approx \\int \\phi\\left(f^* \\sqrt{\\frac{\\pi}{8}}\\right) p(f^*|\\mathbf{y}, \\mathbf{x}^*) \\text{d} f^* = \\Phi\\left(\\frac{\\mu_{f^*}}{\\sqrt{\\frac{8}{\\pi} + \\sigma^2_{f^*}}}\\right) \\tag{3}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Task 4.1**: Go back up the class `GaussianProcessClassification` and complete the implementation of the function `predict_y` using the probit approximation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep model\n",
    "kernel = StationaryIsotropicKernel(squared_exponential)\n",
    "likelihood = BernoulliLikelihood\n",
    "gpc = GaussianProcessClassification(X, y, likelihood, kernel, kappa=3., lengthscale=1.)\n",
    "\n",
    "# predict \n",
    "mu, Sigma = gpc.predict_f(Xstar)\n",
    "p = gpc.predict_y(Xstar)\n",
    "\n",
    "# compute prediction distribution p(f^*|y, x^*) and p(y^*|y, x^*)\n",
    "f_samples = generate_samples(mu[:, None], Sigma, 20).T\n",
    "p_samples = sigmoid(f_samples)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 4))\n",
    "plot_data(ax[0], X, y)\n",
    "plot_data(ax[1], X, y)\n",
    "\n",
    "ax[0].plot(X, gpc.m, 'k.')\n",
    "plot_with_uncertainty(ax[0], Xstar, mu, Sigma, color='g', title='$p(f^*|\\mathbf{y}, x^*)$', num_samples=10)\n",
    "ax[0].legend(ncol=5)\n",
    "\n",
    "ax[1].plot(Xstar, p, 'g-', label='$p(y^*=1|\\mathbf{y}, \\mathbf{x}^*)$')\n",
    "ax[1].plot(Xstar.ravel(), p_samples.T, 'g-', alpha=0.1)\n",
    "ax[1].legend()\n",
    "ax[1].set(title='$p(y^*=1|\\mathbf{y}, \\mathbf{x}^*)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the code has been implemented correct, the code above produces plot of the posterior distribution $p(f^*|\\mathbf{y}, \\mathbf{x}^*)$ and the posterior predictive $p(y^*=1|\\mathbf{y}, \\mathbf{x}^*)$\n",
    "\n",
    "**Task 4.2**: Inspect the plots above. Comment on the behavior of the posterior predictive distribution. Which regions of the input space is dominated by aleatoric uncertainty and which are dominated by epistemic uncertainty? [**Discussion questions**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we will set-up a simple neural network model as a baseline. We will use a two layer network with 20 neuron in each hidden layer. We will impose an i.i.d. Gaussian prior on all the weights, i.e. $p(w_i) \\sim \\mathcal{N}(0, \\alpha^{-1})$ and use a MAP-estimator for the weights. We will not dive into the details of this model, but simply use it as a baseline. The class `NeuralNetworkMAP` implements estimator a MAP-estimator for the neural network model and the code can be found in the module called `exercise6.py` if you are curious.\n",
    "\n",
    "The cell below shows the predictive distribution for the network obtained using the plugin approximation compared with the Guassian process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the NN model with 1 input dimension, 2 hidden layers of 20 neurons each, and 1 output\n",
    "nn = NeuralNetworkMAP(X, y, [1, 20, 20, 1], alpha=1., likelihood=BernoulliLikelihood)\n",
    "\n",
    "# predict using neural network\n",
    "y_nn = nn.predict(Xstar)\n",
    "p_nn = sigmoid(y_nn)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 4))\n",
    "plot_data(ax, X, y)\n",
    "ax.plot(Xstar, p, 'g-', label='$p(y^*=1|\\mathbf{y}, \\mathbf{x}^*)$')\n",
    "ax.plot(Xstar.ravel(), p_samples.T, 'g-', alpha=0.1)\n",
    "ax.set(title='$p(y^*=1|\\mathbf{y}, \\mathbf{x}^*)$ (GP)');\n",
    "ax.plot(Xstar, p_nn, label='$p(y^*=1|\\mathbf{y}, \\mathbf{x}^*)$ (NN-MAP)')\n",
    "ax.legend();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.3**: Compare the predictive distribution for the neural network and the Gaussian process. How do they differ (qualitatively) within the support of the data?\n",
    " Outside the support of the data? [**Discussion question**]\n",
    "\n",
    " *Hint*: Go to the top of the notebook and increase the prediction range for ´Xstar´, e.g. from $\\left[-7.5, 7.5\\right]$ to $\\left[-15, 15\\right]$*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 4.4**: What happens if you increase $\\alpha$ from $1$ to e.g. $10$ or decrease it to $0.01$? Can you explain the result? **[Discussion question]**\n",
    "\n",
    "*Hint: Recall the relationship between Gaussian priors and regularization when using MAP estimates*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5:  Gaussian process classification for 2D data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the MNIST data from week 4 to further study the properties of Gaussian process models.\n",
    "\n",
    "The function in the cell below loads the MNIST data and projects the data to a 2-dimensional space using PCA as we did in week 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = [4, 7]\n",
    "Xtrain, Xtest, ytrain, ytest = load_MNIST_subset('./mnist_subset.npz', digits=digits, subset=500)\n",
    "\n",
    "# make sure dimensions are [N x 1]\n",
    "ytrain = ytrain[:, None]\n",
    "ytest = ytest[:, None]\n",
    "\n",
    "X = np.row_stack((Xtrain, Xtest))\n",
    "y = np.row_stack((ytrain, ytest))\n",
    "\n",
    "# plot\n",
    "def plot_data(ax, title=\"\", xlim=(-5, 5), ylim=(-5, 5)):\n",
    "    ax.plot(Xtrain[:, 0], Xtrain[:, 1], 'ko', markersize=7, label='Training data')\n",
    "    ax.plot(X[y.ravel()==0, 0], X[y.ravel()==0, 1], 'b.', label='Digit %d' % digits[0])\n",
    "    ax.plot(X[y.ravel()==1, 0], X[y.ravel()==1, 1], 'r.', label='Digit %d' % digits[1])\n",
    "    ax.set_xlim((-5,5 ))\n",
    "    ax.set_ylim((-5,5 ))\n",
    "    ax.set(title=title, xlim=xlim, ylim=ylim, xlabel='PC1', ylabel='PC2')\n",
    "    ax.legend()\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,9))\n",
    "plot_data(ax, title='Subset of MNIST data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian process\n",
    "kernel = StationaryIsotropicKernel(squared_exponential)\n",
    "gpc = GaussianProcessClassification(Xtrain, ytrain, BernoulliLikelihood, kernel, kappa=1, lengthscale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "p_train = gpc.predict_y(Xtrain)\n",
    "p_test = gpc.predict_y(Xtest)\n",
    "\n",
    "# make predictions\n",
    "ytrain_hat = 1.0*(p_train > 0.5)\n",
    "ytest_hat = 1.0*(p_test > 0.5)\n",
    "\n",
    "# print results: mean and standard error of the mean\n",
    "print('Training error:\\t%3.2f (%3.2f)' % compute_err(ytrain_hat.ravel(), ytrain.ravel()))\n",
    "print('Test error:\\t%3.2f (%3.2f)' % compute_err(ytest_hat.ravel(), ytest.ravel()))\n",
    "\n",
    "# prepare for plotting\n",
    "x_grid, posterior_y_gpc_eval_mu = eval_density_grid(lambda x: gpc.predict_f(x)[0], P=50, )\n",
    "x_grid, posterior_y_gpc_eval_var = eval_density_grid(lambda x: np.diag(gpc.predict_f(x)[1]), P=50)\n",
    "x_grid, posterior1_gpc_eval = eval_density_grid(gpc.predict_y, P=50, a=-5, b=5)\n",
    "\n",
    "# prepare plot\n",
    "fig, ax = plt.subplots(1, 3, figsize=(25, 6))\n",
    "\n",
    "# plot posterior mean\n",
    "im = ax[0].pcolormesh(x_grid, x_grid, posterior_y_gpc_eval_mu, cmap=plt.cm.RdBu_r, norm=colors.CenteredNorm(), shading='auto')\n",
    "plot_data(ax[0], title=\"Posterior mean of $f^*$\")\n",
    "add_colorbar(im, fig, ax[0])\n",
    "\n",
    "# plot posterior var\n",
    "im = ax[1].pcolormesh(x_grid, x_grid, posterior_y_gpc_eval_var, shading='auto')\n",
    "plot_data(ax[1], title=\"Posterior variance of $f^*$\")\n",
    "add_colorbar(im, fig, ax[1])\n",
    "\n",
    "# plot posterior predictive\n",
    "im = ax[2].pcolormesh(x_grid, x_grid, posterior1_gpc_eval, cmap=plt.cm.RdBu_r, shading='auto', clim=(0, 1))\n",
    "plot_data(ax[2], title=\"Predictive prob for $y^* = 1$\")\n",
    "add_colorbar(im, fig, ax[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.1**: Explain what you see in the 3 panels - explain the difference between the two distributions $p(f^*|\\mathbf{y})$ and $p(y^*|\\mathbf{y})$ [**Discussion question**]\n",
    "\n",
    "**Task 5.2**: What does the values of mean $\\mathbf{m}$ in the Laplace approximation $q(\\mathbf{f}) = \\mathcal{N}(\\mathbf{f}|\\mathbf{m}, \\mathbf{S})$ represent and how does it relate to the figures? [**Discussion question**]\n",
    "\n",
    "**Task 5.3**: Comment on the uncertainties within the support of the data and outside the data. Compare with the equivalent plots for the NN below [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetworkMAP(Xtrain, ytrain, [2, 20, 20, 1], alpha=1., likelihood=BernoulliLikelihood)\n",
    "\n",
    "x_grid, posterior1_nn_eval = eval_density_grid(lambda x: sigmoid(nn.predict(x)), P=50, a=-5, b=5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "im = ax[0].pcolormesh(x_grid, x_grid, posterior1_nn_eval, cmap=plt.cm.RdBu_r, shading='auto')\n",
    "plot_data(ax[0], title='Predictive prob for y = 1 for NN')\n",
    "add_colorbar(im, fig, ax[0])\n",
    "\n",
    "plot_data(ax[1], title='Posterior predictive for GP')\n",
    "im = ax[1].pcolormesh(x_grid, x_grid, posterior1_gpc_eval, cmap=plt.cm.RdBu_r, shading='auto')\n",
    "add_colorbar(im, fig, ax[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02477",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
