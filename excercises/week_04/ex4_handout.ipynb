{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02477 Bayesian Machine Learning - Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as snb\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.stats import beta as beta_dist\n",
    "from scipy.stats import binom as binom_dist\n",
    "from scipy.stats import norm as norm_dist\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "snb.set_style('darkgrid')\n",
    "snb.set_theme(font_scale=1.)\n",
    "\n",
    "def add_colorbar(im, fig, ax):\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im, cax=cax, orientation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will work with probabilistic methods for classification. We will focus on **binary** classification, but everything we discuss can fairly easily be extended to **multi-class** classification. For binary classification we typically observe a dataset $\\mathcal{D} = \\left\\lbrace \\mathbf{x}_n, y_n \\right\\rbrace_{n=1}^N$, where $\\mathbf{x}_n \\in \\mathbb{R}^D$ and $y_n \\in \\left\\lbrace 0, 1 \\right\\rbrace$ are assumed to be input features and label for the $n$'th observation, respectively. Our goal is to use the dataset $\\mathcal{D}$ to reason about a new data point $\\mathbf{x}^* \\in \\mathbb{R}^D$ and its associated label $y^* \\in \\left\\lbrace 0, 1\\right\\rbrace$.\n",
    "\n",
    "We will study two different approaches: the **generative** and the **discriminative** approach. In the former we model the **joint distribution of inputs and outputs**, i.e. $p(\\mathbf{x}, y)$, whereas in the latter we only model the **conditional distribution of the output conditioned on the input**, i.e. $p(y|\\mathbf{x})$. That is, for generative classification we make assumptions on the distribution of the inputs $p(\\mathbf{x})$, either implicitly or explicitly, whereas for discriminative classification we only make assumptions about the distribution of the different outcomes given the specific input. \n",
    "\n",
    "We will also study the Laplace approximation as a computational tool for approximating the posterior distribution of the parameters of a probabilistic model. First, we will study the properties of the Laplace approximation using a model we also know, the beta-binomial model. Afterwards, we will apply the Laplace approximation to the logistic regression model.\n",
    "\n",
    "\n",
    "**Content**\n",
    "\n",
    "- Part 1: Implementing a simple generative classifier\n",
    "- Part 2: Understanding the Laplace approximations\n",
    "- Part 3: Implementing the logistic regression model\n",
    "- Part 4: Implementing the Laplace approximation for logistic regression\n",
    "- Part 5: Approximating the posterior predictive distribution\n",
    "- Part 6: Insights into the posterior predictive distribution via the probit approximation\n",
    "\n",
    "\n",
    "\n",
    "**Note**: The exercise contains several **discussion questions**, which are questions, where are supposed to actively experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs. Instead of proving the full description for every discussion question, we simply tag it with: [**Discussion question**] after the question.\n",
    "\n",
    "We will be working with the MNIST dataset (more information here: http://yann.lecun.com/exdb/mnist/), which consists of digital images of digits from 0-9. Each image is 28x28 pixels and is represented in a vectorized manner such that each image is represented as a vector in $\\mathbb{R}^{784}$. Specifically, we will build classifiers for distinguishing between 8s (encoded as $y=0$) and 9s (encoded as $y=1$), a binary classification problem. In order to visualize the resulting dataset, we use **principal component analysis** (PCA) to reduce the dimensionality to $D = 2$. We won't cover PCA in this course, but you can read more about it in Section 20.1 in Murphy1 or in Section 12.1 in the Bishop if you are unfamiliar with it.\n",
    "\n",
    "The matrices $\\textbf{I}_{\\text{train}} \\in \\mathbb{R}^{197 \\times 784}$ and $\\textbf{X}_{\\text{train}} \\in \\mathbb{R}^{197 \\times 2}$ contains the raw pixel values and the low-dimensional PCA representation, respectively. Note that we will use a fairly small training set to make the problem more challenging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = np.load('./mnist_subset_89.npz')\n",
    "Itrain, Itest  = data['Itrain'], data['Itest']\n",
    "Xtrain, Xtest = data['Xtrain'], data['Xtest']\n",
    "ytrain, ytest = data['ytrain'], data['ytest']\n",
    "digits = data['digits']\n",
    "\n",
    "fig, ax = plt.subplots(1, 10, figsize=(20, 6))\n",
    "for idx_plot, i in enumerate(np.random.choice(range(len(Xtrain)), size=10)):\n",
    "    ax[idx_plot].imshow(Itrain[i].reshape((28, 28)), cmap=plt.cm.gray)\n",
    "    ax[idx_plot].set_title('Label: %d' % ytrain[i])\n",
    "    ax[idx_plot].axis('off')\n",
    "\n",
    "print('Shape of Itrain:', Itrain.shape)\n",
    "print('Shape of Xtrain:', Xtrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work in the 2-dimensional PCA space in the rest of the exercise for easier visualization, but nothing prevents us from working on the original data space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(ax, X, y, alpha=0.8, title=None):\n",
    "\n",
    "    ax.plot(X[y==0, 0], X[y==0, 1], 'b.', label='y = 0 (digit=8)')\n",
    "    ax.plot(X[y==1, 0], X[y==1, 1], 'r.', label='y = 1 (digit=9)')\n",
    "    ax.set(xlabel='PC1', ylabel='PC2')\n",
    "    ax.legend()\n",
    "    \n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "plot_data(ax[0], Xtrain, ytrain, title='Training set')\n",
    "plot_data(ax[1], Xtest, ytest, title='Test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:  Implementing a simple generative classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the exercise, we will study a simple **generative classifier** given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y = k | \\textbf{x}) = \\frac{p(\\textbf{x}|y=k)p(y=k)}{p(\\textbf{x})},\n",
    "\\end{align*}$$\n",
    "\n",
    "where\n",
    "- $k$ can take the values $0$ or $1$ since we are dealing with binary classification\n",
    "- $p(y = k | \\textbf{x})$ the posterior probability that the observation $y$ belongs to class $k$ given the input point $\\textbf{x}$\n",
    "- $p(\\textbf{x}|y=k)$ is the class-conditional distribution for class $k$\n",
    "- $p(y=k)$ is the prior class probability for the $k$'th class\n",
    "- $p(\\textbf{x})$ is the marginal density of the data\n",
    "\n",
    "The **prior probability**, $p(y=k)$ of an observation belonging to class $k$ is given by $p(y=k)=\\pi_k \\in \\left[0,1\\right]$ and satisfies $\\sum_{k} \\pi_k = 1$.\n",
    "\n",
    "For the **class-conditional distributions** we will **assume** bivariate normal distributions with mean $\\mathbf{m}_k$ and covariance matrix $\\textbf{S}_k$. That is,\n",
    "\\begin{align*}\n",
    "p(\\textbf{x}|y=k) = \\mathcal{N}(\\textbf{x}_n|\\mathbf{m}_k, \\textbf{S}_k).\n",
    "\\end{align*}\n",
    "\n",
    "We can derive the marginal data distribution $p(\\textbf{x})$ via the sum rule by marginalizing out $y$ from $p(\\textbf{x}, y)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\textbf{x}) = \\sum_{y} p(\\textbf{x}, y) = \\sum_{y} p(\\textbf{x}|y)p(y) =p(y=0)\\mathcal{N}(\\textbf{x}|\\mathbf{m}_0, \\textbf{S}_0) + p(y=1)\\mathcal{N}(\\textbf{x}|\\mathbf{m}_1, \\textbf{S}_1).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "We can estimate this probability using the training data by estimating the fraction data point belong to each of the classes, e.g.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\pi_1 = p(y=1) &\\approx \\frac{\\text{number of training point with label 1}}{\\text{total number of training points}} = \\frac{1}{N} \\sum_{n=1}^N y_n.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We only need to estimate the prior probability for one of the two classes since $p(y=0) = 1 - p(y=1)$.\n",
    "\n",
    "The posterior distribution $p(y|\\textbf{x})$ gives us a number in the unit interval. We can turn this into a classification rule, by classifying an input point $x$ as the class $k$ that maximizes the posterior class probability. That is,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y}_n = \\arg\\max\\limits_{k} p(y_n = k | x_n)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Task 1.1**: Estimate the prior probability of $p(y=1)$ and $p(y=0)$ using the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2**: Estimate and report the parameters of the class-conditional distributions $p(\\mathbf{x}|y=k)$, i.e. $\\left\\lbrace \\textbf{m}_0, \\textbf{m}_1, \\textbf{S}_0, \\textbf{S}_1 \\right\\rbrace$, using the training data\n",
    "\n",
    "*Hints*: This is meant to be an easy exercise and the functions: *np.mean* and *np.cov* might be handy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3**: Plot the contours of the two class-conditional distributions superimposed the training data. Comment on how well the assumptions fit the data.\n",
    "\n",
    "*Hints:*\n",
    "- *The PDF and log PDF of a multivariate Gaussian can be evaluated using mvn.pdf(x, mean, cov) and mvn.logpdf(x, mean, cov)*\n",
    "- *Below you are given a function for evaluating a function on a 2D Cartisian grid. You can use this for evaluating the distributions efficiently, but there are many other ways to implement this.*\n",
    "- *For the contour plots, you can find inspiration in the function *plot_distribution* from last week.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_density_grid(density_fun, dim1_array, dim2_array):\n",
    "    \"\"\" evaluates the function density_fun on two 2d grid formed by the values in the two arrays: dim1_array and dim2_array \"\"\"\n",
    "    X1, X2 = np.meshgrid(dim1_array, dim2_array, indexing='ij')\n",
    "    XX = np.column_stack((X1.ravel(), X2.ravel()))\n",
    "    return density_fun(XX).reshape((len(dim1_array), len(dim2_array)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4**: Implement a function for evaluating the posterior distribution $p(y=1|\\textbf{x})$ and the marginal data distribution $p(\\textbf{x})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.5**: Create a plot with three subfigures: 1) plotting the class-conditional data distributions superimposed on the data, 2) plot the marginal data distribution superimposed on the data, and 3) plot the posterior distribution superimposed on the data. Comment on the plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the **accuracy** and **ELPD** (expected log predictive density) to quantify the performance of the classifier is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{ELPD}(\\textbf{y}^*, \\textbf{p}^*) = \\frac{1}{M} \\sum_{i=1}^M \\log \\left(\\text{Ber}(y^*_i|p^*_i)\\right),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\textbf{y}^*$ in this case is the vector of ground truth values and $\\textbf{p} ^*$ is a vector of predictive probabilities, i.e. $p^*_i = p(y^*_i=1|\\textbf{x}^*_i)$. Accuracy only measures how often we are correct and incorrect, whereas the ELPD also takes in uncertainty into account. For both metrics, larger values are better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating accuracy\n",
    "def accuracy(y_true, p_hat, threshold=0.5):\n",
    "    return np.mean(y_true.ravel() == 1.0*(p_hat.ravel() > threshold))\n",
    "\n",
    "log_ber = lambda y, p: (1-y)*np.log(1-p+1e-16) + y*np.log(p+1e-16)\n",
    "\n",
    "# function for evaluating the ELPD\n",
    "def elpd(y_true, p_hat):\n",
    "    return np.mean(binom_dist.logpmf(y_true.ravel(), n=1, p=p_hat.ravel()))\n",
    "\n",
    "# example\n",
    "y_example = np.array([1, 1, 0, 0, 0])\n",
    "p_example1 = np.array([0.95, 0.95, 0.95, 0.95, 0.1])\n",
    "p_example2 = np.array([0.95, 0.95, 0.51, 0.51, 0.1])\n",
    "\n",
    "print(f' Acc(y, p1) =  {accuracy(y_example, p_example1):3.2f}')\n",
    "print(f' Acc(y, p2) =  {accuracy(y_example, p_example2):3.2f}')\n",
    "print(f'ELPD(y, p1) = {elpd(y_example, p_example1):3.2f}')\n",
    "print(f'ELPD(y, p2) = {elpd(y_example, p_example2):3.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.6**: Evaluate the accuracy and elpd for the training set and test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concludes the part on **generative modelling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:  Understanding the Laplace approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can study the **discriminative approach** for classification, we first need to study the **Laplace approximation** because this will be our main workhorse for (approximate) posterior inference. To gain some intuition, we will first study the Laplace approximation in the context of a model we already now and understand, the Beta-binomial model, which is given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "y|\\theta &\\sim \\text{Bin}(N, \\theta),\\\\\n",
    "\\theta &\\sim \\text{Beta}(\\alpha_0, \\beta_0)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "with joint distribution $p(y, \\theta) = p(y|\\theta)p(\\theta)$ and $\\alpha_0, \\beta_0 > 0$ are fixed hyperparameters. For this specific model, we already know that the true posterior of $\\theta$ is another Beta distribution given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\theta|y) = \\text{Beta}(\\theta|\\alpha_0 + y, \\beta_0 + N -y).\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence, we can approximate $p(\\theta|y)$ with a Laplace approximation $q(\\theta) = \\mathcal{N}(\\theta|\\textbf{m}, \\textbf{V})$ and the study the accuracy of the $q(\\theta)$ by comparing it to the true solution $p(\\theta|y)$.\n",
    "Recall, the Laplace approximation is obtained by making a second order Taylor approximation of the log joint distribution, i.e. $f(\\theta) = \\log p(\\theta, y)$, around the mode, i.e. $\\theta_{\\text{MAP}} = \\arg\\max_{\\theta} p(\\theta|y)$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\log f(\\theta) \\approx \\log f(\\theta_{\\text{MAP}}) - \\frac{1}{2}(\\theta-\\theta_{\\text{MAP}})A(\\theta - \\theta_{\\text{MAP}})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "leading to the Laplace approximation\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\theta|y) \\approx q(\\theta) = \\mathcal{N}(\\theta|\\theta_{\\text{MAP}}, A^{-1}).\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence, to set-up the Laplace approximation, our first order of business is to get a handle on the joint distribution:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y, \\theta) = p(y|\\theta)p(\\theta).\n",
    "\\end{align*}$$\n",
    "\n",
    "**Task 2.1**: Show that the log joint distribution is given by $\\ln p(y, \\theta) = (\\alpha_0 + y - 1)  \\ln \\theta +  (\\beta_0 + N-y -1)\\ln (1-\\theta) + \\text{constant}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next job is to compute the first and second order derivatives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2**: Show that the 1st and 2nd derivative of $\\ln p(y, \\theta)$ wrt. $\\theta$ is given by \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial }{\\partial \\theta}\\ln p(y, \\theta) &= (\\alpha_0 + y - 1)  \\frac{1}{\\theta} -(\\beta_0 + N-y -1) \\frac{1}{1-\\theta}\\\\\n",
    "\\frac{\\partial^2 }{\\partial \\theta^2}\\ln p(y, \\theta) &= -(\\alpha_0 + y - 1) \\frac{\\partial }{\\partial \\theta} \\frac{1}{\\theta^2} -(\\beta_0 + N-y -1) \\frac{1}{(1-\\theta)^2}\n",
    "\\end{align*}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below implements functions for plotting exact posterior PDF as well as computing its mean and variance. Initially, we will assume $N = 12$ and $y=2$. It also contains incomplete functions for evaluating the log joint, the gradient, and hessian, which you are going to implement later. But first, let's plot the posterior density for our specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaBinomial(object):\n",
    "\n",
    "    def __init__(self, y, N, alpha0=1., beta0=1.):\n",
    "        # data\n",
    "        self.y, self.N = y, N\n",
    "        # hyperparameters\n",
    "        self.alpha0, self.beta0 = alpha0, beta0\n",
    "        # true posterior parameters\n",
    "        self.alpha, self.beta = alpha0 + y, beta0 + N - y\n",
    "\n",
    "    @property\n",
    "    def posterior_mean(self):\n",
    "        \"\"\" compute the exact posterior mean \"\"\"\n",
    "        return self.alpha/(self.alpha + self.beta)\n",
    "    \n",
    "    @property\n",
    "    def posterior_variance(self):\n",
    "        \"\"\" compute the exact posterior variance \"\"\"\n",
    "        return self.alpha*self.beta/((self.alpha+self.beta)**2*(self.alpha+self.beta+1))\n",
    "\n",
    "    def pdf(self, theta):\n",
    "        return beta_dist.pdf(theta, self.alpha, self.beta)\n",
    "    \n",
    "    def log_joint(self, theta_):\n",
    "        \"\"\" evaluates and return the log joint p(y, theta_) \"\"\"\n",
    "        return <insert code here>\n",
    "\n",
    "    def grad(self, theta_):\n",
    "        \"\"\" evaluates and return the gradient of the log joint p(y, theta) write to theta evaluated at theta_ \"\"\"\n",
    "        return <insert code here>\n",
    "\n",
    "    def hessian(self, theta_):\n",
    "        \"\"\" evaluates and return the hessian of the log joint p(y, theta) write to theta evaluated at theta_ \"\"\"\n",
    "        return <insert code here>\n",
    "\n",
    "\n",
    "# specify data and instantiate model\n",
    "N = 12\n",
    "y = 2\n",
    "model = BetaBinomial(y, N)\n",
    "\n",
    "# plot exact posterior density\n",
    "theta_grid = np.linspace(-0.25, 1.25, 1000)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(theta_grid, model.pdf(theta_grid), label='$p(\\\\theta|y)$', linewidth=2)\n",
    "ax.set(xlabel='$\\\\theta$', ylabel='Density')\n",
    "ax.legend()\n",
    "\n",
    "# report exact posterior mean and variance\n",
    "print(f'The exact posterior mean is {model.posterior_mean:4.3f}')\n",
    "print(f'The exact posterior variance is {model.posterior_variance:4.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3**: Go back up to the implementation of the class *BetaBinomial* and complete the implementation of the log joint as well as its gradient as hessian.\n",
    "\n",
    "*Hints: The sanity below helps to verify your code. However, passing the test does not guarantee your code is correct.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check of implementation\n",
    "model = BetaBinomial(y=1, N=3, alpha0=1, beta0=1)\n",
    "assert np.allclose(model.grad(1/4), 4/3), \"Something appears to be wrong with your implementation of the gradient. Check your implementation.\"\n",
    "assert np.allclose(model.grad(1/2), -2), \"Something appears to be wrong with your implementation of the gradient. Check your implementation.\"\n",
    "assert np.allclose(model.grad(3/4), -6.666666666666667), \"Something appears to be wrong with your implementation of the gradient. Check your implementation.\"\n",
    "assert np.allclose(model.hessian(1/4), -19.555555555555557), \"Something appears to be wrong with your implementation of the hessian. Check your implementation.\"\n",
    "assert np.allclose(model.hessian(1/2), -12), \"Something appears to be wrong with your implementation of the hessian. Check your implementation.\"\n",
    "assert np.allclose(model.hessian(3/4), -33.77777777777778), \"Something appears to be wrong with your implementation of the hessian. Check your implementation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything was implemented correctly, we can use almost any off-the-shelf optimizer for locating the mode of the posterior. We recommend using the *minimize* function from *scipy.optimize*. Note that our goal is to find the point that maximizes the posterior, whereas the *minimize*-function is designed for minimization and therefore, we need to flip the sign of the log_joint and its gradient when optimizing. Below we provide an example of how to use the optimizer in scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify data and instantiate model\n",
    "N = 12\n",
    "y = 2\n",
    "model = BetaBinomial(y, N)\n",
    "\n",
    "# optimize\n",
    "results = minimize(lambda x: -model.log_joint(x), jac= lambda x: -model.grad(x), x0=0.5, bounds=[(1e-10,1-1e-10)])\n",
    "\n",
    "# print full results\n",
    "print(results)\n",
    "\n",
    "if results.success:\n",
    "    print('Optimization succeded!\\n')\n",
    "    print(f'\\tSolution: theta         = {results.x[0]:+4.3}')\n",
    "    print(f'\\t|gradient| at solution  = {results.jac[0]:+4.3e}')\n",
    "else:\n",
    "    print('Optimization failed. Check your implementation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we turn our attention to the Laplace approximation itself.\n",
    "\n",
    "**Task 2.4**: Complete the LaplaceApproximation1D class below. You need to complete the following functions:  *construct_approximation*, *taylor_approx*, *pdf*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions implementing the pdf of a normal distribution\n",
    "log_npdf = lambda x, m, v: -(x-m)**2/(2*v) - 0.5*np.log(2*np.pi*v)\n",
    "npdf = lambda x, m, v: np.exp(log_npdf(x, m, v))\n",
    "\n",
    "class LaplaceApproximation1D(object):\n",
    "\n",
    "    def __init__(self, model, init_param=0):\n",
    "        self.model = model\n",
    "        self.param_MAP = None\n",
    "        self.param_Hessian = None\n",
    "        self.mean = None\n",
    "        self.variance = None\n",
    "        self.construct_approximation()\n",
    "\n",
    "    def construct_approximation(self):\n",
    "\n",
    "        # get MAP solution by optimization\n",
    "        opt_results = minimize(lambda x: -model.log_joint(x), jac= lambda x: -model.grad(x), x0=0.5, bounds=[(1e-10,1-1e-10)])\n",
    "        if not opt_results.success:\n",
    "            print(opt_results)\n",
    "            raise ValueError(\"Optimization failed. Printing the details from optimization for debugging\")\n",
    "\n",
    "        # evaluation Hessian at the mode\n",
    "        self.param_MAP = <insert code here>\n",
    "        self.Hessian = <insert code here>\n",
    "        self.mean = <insert code here>\n",
    "        self.variance = <insert code here>\n",
    "\n",
    "    def taylor_approx(self, theta):\n",
    "        \"\"\"\" computes and returns the 2nd order Taylor approximation for f(theta) evalauted at theta \"\"\"\n",
    "        return <insert code here>\n",
    "        \n",
    "    def pdf(self, theta):\n",
    "        \"\"\" evaluates the PDF of the Laplace approximation q(theta) at input parameter value theta \"\"\"\n",
    "        return <insert code here>\n",
    "\n",
    "\n",
    "# specify data and instantiate model\n",
    "N = 12\n",
    "y = 2\n",
    "model = BetaBinomial(y, N)\n",
    "laplace_approx = LaplaceApproximation1D(model)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1,2, figsize=(20, 6))\n",
    "\n",
    "theta_space_unit = np.linspace(1e-6, 1-1e-6, 1000)\n",
    "theta_space = np.linspace(-0.25, 1.25, 1000)\n",
    "ax[0].plot(theta_space_unit, model.log_joint(theta_space_unit), 'g', label='Log joint $p(y, \\\\theta)$')\n",
    "ax[0].plot(theta_space, laplace_approx.taylor_approx(theta_space), color='r', label='2nd order taylor expansion')\n",
    "ax[0].axvline(laplace_approx.mean, color='k', linestyle='--', alpha=0.2, label='$\\\\theta_{MAP}$')\n",
    "ax[0].set(xlabel='$\\\\theta$')\n",
    "ax[0].legend();\n",
    "ax[0].set_title('Log joint vs $\\\\theta$', fontweight='bold');\n",
    "\n",
    "ax[1].plot(theta_space, model.pdf(theta_space),  'g', label='True posterior density for $\\\\theta$')\n",
    "ax[1].plot(theta_space, laplace_approx.pdf(theta_space), color='r', label='Laplace approximation $q(\\\\theta)$')\n",
    "ax[1].axvline(laplace_approx.mean, color='k', linestyle='--', alpha=0.2, label='$\\\\theta_{MAP}$')\n",
    "ax[1].set(xlabel='$\\\\theta$')\n",
    "ax[1].legend();\n",
    "ax[1].set_title('Posterior density for $\\\\theta$', fontweight='bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5**: How well does the Laplace approximation $q(\\theta)$ resemble the true posterior? Experiment with different number of trials $N$ and successes $y$ and explore how the change affects the quality of the approximation. Try the following pairs $(y, N) \\in \\left\\lbrace (1, 3), (10, 30), (10, 300), (1, 20) \\right\\rbrace$. Does the Laplace approximation 'respect' the domain of the parameter $\\theta$?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3:  Implementing the logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipped with the Laplace approximation, we will now turn our attention towards the Bayesian logistic regression model for binary classification. Recall, the model is defined as\n",
    "\n",
    "$$\\begin{align*}\n",
    "y_n|\\mathbf{w}, \\mathbf{x}_n &\\sim \\text{Ber}(\\sigma(f(\\textbf{x}_n)))\\\\\n",
    "\\mathbf{w} &\\sim \\mathcal{N}(0, \\alpha^{-1}\\mathbf{I}),\n",
    "\\end{align*}$$\n",
    "\n",
    "where $f(\\textbf{x})$ is a linear model with parameters $\\textbf{w}$ given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Our goal for the exercise is two-fold: 1) we want to obtain the posterior distribution $p(\\textbf{w}|\\mathbf{y})$ to be able reason about the parameters in the linear model, and 2) we want to obtain the posterior predictive distribution $p(y^*|\\textbf{y}, \\textbf{x}^*)$ for a new input point $\\textbf{x}^*$. Since logistic regression is a **non-conjugate** model, we need to resort to the Laplace approximation for approximate inference $p(\\textbf{w}|\\textbf{y}) \\approx q(\\textbf{w}) = \\mathcal{N}(\\mathbf{w}|\\mathbf{m}, \\mathbf{S})$.\n",
    "\n",
    "First, we will write up the log joint distribution using the short-hand notation $f_n \\equiv f(\\mathbf{x}_n) = \\mathbf{w}^T \\mathbf{x}_n$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\log p(\\mathbf{y}, \\mathbf{w}) &= \\log p(\\mathbf{y}|\\mathbf{w}) + \\log p(\\mathbf{w})\\\\\n",
    "&= \\sum_{n=1}^N \\log \\text{Ber}(y_n|\\sigma(f_n))+  \\log \\mathcal{N}(\\mathbf{w}|0, \\alpha^{-1}\\mathbf{I})\\\\\n",
    "&= \\sum_{n=1}^N \\left[(1-y_n)\\log(1-\\sigma(f_n)) + y_n\\log(\\sigma(f_n)) \\right] - \\frac{\\alpha}{2}\\mathbf{w}^T \\mathbf{w} + \\text{const.}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement the Laplace approximation, we need gradient and Hessian of the log joint distribution.\n",
    "From this distribution, we can derive the gradient and Hessian. Calculating the gradient yields:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla_{\\mathbf{w}}\\log p(\\mathbf{y}, \\mathbf{w}) &= \\sum_{n=1}^N \\left[(1-y_n)\\nabla_{\\mathbf{w}}\\log(\\sigma(-f_n)) + y_n\\nabla_{\\mathbf{w}}\\log(\\sigma(f_n)) \\right] - \\frac{\\alpha}{2}\\nabla_{\\mathbf{w}}\\mathbf{w}^T \\mathbf{w} + \\text{const.}\\\\\n",
    "%\n",
    "&= \\sum_{n=1}^N \\left[(1-y_n)\\nabla_{\\mathbf{w}}\\log(\\sigma(-f_n)) + y_n\\nabla_{\\mathbf{w}}\\log(\\sigma(f_n)) \\right] - \\alpha \\mathbf{w} + \\text{const.}\n",
    "\\end{align*}$$\n",
    "\n",
    "By repeated use of the chain-rule, we evaluate the derivative of $\\nabla_{\\mathbf{w}}\\log(\\sigma(f_n))$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mathbf{w}}\\log(\\sigma(f_n)) &= \\nabla_{\\mathbf{w}} \\log \\frac{1}{1+e^{-f_n}} = -\\nabla_{\\mathbf{w}} \\log (1+e^{-f_n})\\\\\n",
    "&= - \\frac{1}{1+e^{-f_n}} \\nabla_{\\mathbf{w}}\\left(1+e^{-f_n}\\right) \\\\\n",
    "&= \\frac{1}{1+e^{-f_n}}e^{-f_n} \\nabla_{\\mathbf{w}} f_n \\\\\n",
    "&= \\frac{1}{1+e^{-f_n}}e^{-f_n} \\nabla_{\\mathbf{w}} \\bm{w}^T \\mathbf{x}_n \\\\\n",
    "&= \\frac{e^{-f_n}}{1+e^{-f_n}}\\mathbf{x}_n \\\\\n",
    "&= \\left[1-\\sigma(f_n)\\right]\\mathbf{x}_n \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and a similar line of calculations yields\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mathbf{w}}\\log(1-\\sigma(f_n)) &=  -\\sigma(f_n) \\textbf{x}_n.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Substituting back into the expression yields:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla_{\\mathbf{w}}\\log p(\\mathbf{y}, \\mathbf{w}) &= \\sum_{n=1}^N \\left[-(1-y_n)\\sigma(f_n) \\textbf{x}_n + y_n\\left[1-\\sigma(f_n)\\right]\\mathbf{x}_n\\right] -\\alpha \\mathbf{w} + \\text{const.}\\\\\n",
    "%\n",
    "&= \\sum_{n=1}^N \\left[\\sigma(f_n)+y_n\\sigma(f_n) + y_n-y_n\\sigma(f_n)\\right]\\mathbf{x}_n - \\alpha \\mathbf{w} + \\text{const.}\\\\\n",
    "%\n",
    "&= -\\sum_{n=1}^N \\left[\\sigma(f_n) - y_n\\right]\\mathbf{x}_n - \\alpha \\mathbf{w} + \\text{const.}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the Hessian\n",
    "$$\\begin{align*}\n",
    "\\mathcal{H}(\\textbf{w})_{ij} = \\frac{\\partial^2}{\\partial w_i \\partial w_j} \\log p(\\mathbf{y}, \\mathbf{w}) &= \\frac{\\partial}{\\partial w_i}  \\left[ -\\sum_{n=1}^N \\left[\\sigma(f_n) - y_n\\right]\\mathbf{x}_n - \\alpha \\mathbf{w}\\right]_j\\\\\n",
    "%\n",
    "&= -\\sum_{n=1}^N \\frac{\\partial}{\\partial w_i} \\left[\\sigma(f_n) - y_n\\right]x_{n,j} - \\alpha w_j\\\\\n",
    "%\n",
    "&= -\\sum_{n=1}^N  \\frac{\\partial}{\\partial w_i}\\sigma(f_n)x_{n,j} - \\alpha w_j\\\\\n",
    "%\n",
    "&= -\\sum_{n=1}^N  (1-\\sigma(f_n))\\sigma(f_n)x_{n,i}x_{n,j} - \\alpha w_j\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "and hence,\n",
    "$$\\begin{align*}\n",
    "\\mathcal{H}(\\textbf{w}) = \\nabla^2_{\\textbf{w}} \\log p(\\mathbf{y}, \\mathbf{w}) = -\\sum_{n=1}^N (1-\\sigma(f_n))\\sigma(f_n) \\textbf{x}_n\\textbf{x}_n^T - \\alpha \\textbf{I} = -\\textbf{X}\\textbf{S}\\textbf{X}^T - \\alpha \\textbf{I},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\textbf{S} \\in \\mathbb{R}^{N \\times N}$ is a diagonal matrix with $S_{nn} = (1-\\sigma(f_n))\\sigma(f_n)$ and $\\textbf{X} \\in \\mathbb{R}^{N \\times D}$ is the design matrix such  that $\\textbf{x}_n$ will be the $n$'th row of $\\textbf{X}$.\n",
    "\n",
    "Recall, sometimes we want to use a **feature transformation** $\\phi(\\textbf{x})$ to model non-linear dependencies, e.g. $\\phi(\\textbf{x}_n) = \\begin{bmatrix} 1 & x_{n,1} & x_{n,2} & x_{n,1}^2 & x_{n,2}^2 \\end{bmatrix}$ with $f(\\mathbf{x}_n) = \\mathbf{w}^T \\phi(\\mathbf{x}_n)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting purpose, we will need a class for evaluating function on 2D grids. This is similar to the Grid2D-class used in exercise2, but now we have also equipped it with a function for plotting heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid2D(object):\n",
    "    \"\"\" helper class for evaluating the function func on the grid defined by (dim1, dim2)\"\"\"\n",
    "\n",
    "    def __init__(self, dim1, dim2, func, name=\"Grid2D\"):\n",
    "        self.dim1 = dim1\n",
    "        self.dim2 = dim2\n",
    "        self.grid_size = (len(self.dim1), len(self.dim2))\n",
    "        self.dim1_grid, self.dim2_grid = np.meshgrid(dim1, dim2, indexing='ij')\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "        \n",
    "        # evaluate function on each grid point\n",
    "        params_flat= np.column_stack((self.dim1_grid.flatten(), self.dim2_grid.flatten()))\n",
    "        self.values = self.func(params_flat).squeeze().reshape(self.grid_size)\n",
    "\n",
    "    def plot_contours(self, ax, color='b', num_contours=10, f=lambda x: x, alpha=1.0, title=None):\n",
    "        ax.contour(self.dim1, self.dim2, f(self.values).T, num_contours, colors=color, alpha=alpha)\n",
    "        ax.set(xlabel='$w_1$', ylabel='$w_2$')\n",
    "        ax.set_title(self.name, fontweight='bold')\n",
    "\n",
    "    def plot_heatmap(self, ax, f=lambda x: x, clim=[0, 1], colorbar=False):\n",
    "        img = ax.pcolormesh(self.dim1, self.dim2, f(self.values).T, cmap=plt.cm.RdBu_r, clim=clim)\n",
    "        ax.set(xlabel='$w_1$', ylabel='$w_2$')\n",
    "        ax.set_title(self.name, fontweight='bold')\n",
    "        if colorbar:\n",
    "            add_colorbar(img, ax.get_figure(), ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1**: Below you are given a template for a logistic regression implementation. Complete the implementation of the gradient and hessian using the equations above.\n",
    "\n",
    "If the gradient is implemented correct, the cell below will locate the MAP solution using gradient-based optimization and plot the MAP solution on the of the contours of the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1./(1 + np.exp(-x))\n",
    "\n",
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self, X, y, feature_transformation=lambda x: x, alpha=1.):\n",
    "        \n",
    "        # store data and hyperparameters\n",
    "        self.X0 = X\n",
    "        self.y = y\n",
    "        self.alpha = alpha\n",
    "        self.feature_transformation = feature_transformation\n",
    "\n",
    "        # apply feature transformation and standardize\n",
    "        self.X = feature_transformation(self.X0)\n",
    "        self.X_mean = np.mean(self.X, 0)\n",
    "        self.X_std = np.std(self.X, 0)\n",
    "        self.X_std[self.X_std == 0] = 1.\n",
    "        self.X = self.preprocess(X)\n",
    "\n",
    "        # store number of training data and number of featurs\n",
    "        self.N, self.D = self.X.shape\n",
    "        \n",
    "        # get MAP by optimization\n",
    "        self.w_MAP = self.get_MAP()\n",
    "\n",
    "    def preprocess(self, X_):\n",
    "        X = self.feature_transformation(X_)\n",
    "        return (X - self.X_mean)/self.X_std\n",
    "        \n",
    "    def predict(self, X, w):\n",
    "        \"\"\" evaluates sigma(f(X)) \"\"\"\n",
    "        f = w@X.T\n",
    "        return sigmoid(f)\n",
    "    \n",
    "    def log_joint(self, w):\n",
    "        \"\"\"\n",
    "            evaluates log joint, i.e. log p(y, w), for each row in w.\n",
    "            w is expected to be of shape [M, D], where D is the number of parameters in the model and M is the number of points to evaluated\n",
    "        \"\"\"\n",
    "        p = self.predict(self.X, w)\n",
    "        log_prior = np.sum(log_npdf(w, 0, 1./self.alpha), axis=1)\n",
    "        log_lik = binom_dist.logpmf(self.y, p=p, n=1)\n",
    "        log_joint = log_prior + log_lik.sum(axis=1)\n",
    "\n",
    "        return log_joint\n",
    "    \n",
    "    def hessian(self, w):\n",
    "        \"\"\" Returns hessian of log joint evaluated at w \n",
    "            Input:   w       (shape: [1, D])\n",
    "            Returns: H       (shape: [D, D])            \"\"\"\n",
    "        \n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "        \n",
    "        assert H.shape == (self.D, self.D), f\"The shape of the Hessians appears to be wrong. Expected shape ({self.D}, {self.D}), but received {H.shape}. Check your implementation\"\n",
    "        return H\n",
    "\n",
    "    def grad(self, w):\n",
    "        \"\"\" Returns gradient of log joint evaluated at w \n",
    "            Input:   w          (shape: [1, D])\n",
    "            Returns: grad       (shape: [1, D])            \"\"\"\n",
    "        \n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "\n",
    "        assert grad.shape == (1, self.D), f\"The shape of the gradient appears to be wrong. Expected shape (1, {self.D}), but received {grad.shape}. Check your implementation\"\n",
    "        return grad\n",
    "  \n",
    "    def get_MAP(self):\n",
    "        \"\"\" returns MAP estimate obtained by maximizing the log joint \"\"\"\n",
    "        init_w = np.zeros(self.D)\n",
    "        results = minimize(lambda x: -self.log_joint(x[None, :]), jac=lambda x: -self.grad(x[None, :]).flatten(), x0=init_w)\n",
    "        if not results.success:\n",
    "            print(results)\n",
    "            raise ValueError('Optization failed')\n",
    "        \n",
    "        w_MAP = results.x \n",
    "        return w_MAP\n",
    "    \n",
    "        \n",
    "# instantiate model\n",
    "model = LogisticRegression(Xtrain, ytrain)\n",
    "\n",
    "# function for making prediction using w_MAP point estimate\n",
    "pred_MAP = lambda x: model.predict(model.preprocess(x), model.w_MAP)\n",
    "\n",
    "# prep grid\n",
    "dim1, dim2 = np.linspace(-6, 6, 150), np.linspace(-6, 6, 149)\n",
    "grid_log_joint = Grid2D(dim1, dim2, model.log_joint, name='Posterior distribution $p(w|y)$')\n",
    "grid_predictions = Grid2D(dim1, dim2, pred_MAP, name='$p(y^*=1|y, w_{MAP})$')\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "grid_log_joint.plot_contours(ax[0], f=np.exp)\n",
    "ax[0].plot(model.w_MAP[0], model.w_MAP[1], 'r.', label='$w_{MAP}$ (via optimization)')\n",
    "ax[0].legend()\n",
    "\n",
    "plot_data(ax[1], Xtrain, ytrain, 'Training data')\n",
    "grid_predictions.plot_heatmap(ax[1], colorbar=True)\n",
    "ax[1].set(xlabel='$x_1$', ylabel='$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using feature transformations, we can model non-linear decision boundaries, but we cannot easily visualize the posterior anymore (why not?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple polymials as feature transformation\n",
    "def feature_transformation(x, order=1):\n",
    "    return np.column_stack([x**m for m in range(order+1)])\n",
    "\n",
    "# prep grid\n",
    "dim1, dim2 = np.linspace(-6, 6, 150), np.linspace(-6, 6, 149)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 4))\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "    # instantiate model\n",
    "    model = LogisticRegression(Xtrain, ytrain, feature_transformation=lambda x: feature_transformation(x, order=i))\n",
    "\n",
    "    # function for making prediction using w_MAP point estimate\n",
    "    pred_MAP = lambda x: model.predict(model.preprocess(x), model.w_MAP)\n",
    "\n",
    "    # evaluate prediction for grid\n",
    "    grid_predictions = Grid2D(dim1, dim2, pred_MAP, name='$p(y^*=1|y, w_{MAP})$')\n",
    "\n",
    "    # plot\n",
    "    plot_data(ax[i], Xtrain, ytrain, 'Training data')\n",
    "    grid_predictions.plot_heatmap(ax[i], colorbar=True if i == 3 else False)\n",
    "    ax[i].set(xlabel='$x_1$', ylabel='$x_2$')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4:  Implementing the Laplace approximation for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to implement the Laplace approximation. We have already done most of the hard word, now we simply need to combine all the pieces. For the purpose of validating our implementation, we will temporarily remove the feature transformation again.\n",
    "\n",
    "**Task 4.1**: Complete the implementation the Laplace approximation below. Verify your implementation below generating 200 samples from the posterior, $\\textbf{w}^i \\sim q(\\textbf{w})$ and the plot the samples on top of the contours of posterior distribution. You should expect to see samples covering the contours of the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceApproximation(object):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        \"\"\" implements a laplace approximain q(w) = N(m, S), where m is the posterior mean and S is the posterior covariance \"\"\"\n",
    "\n",
    "        # store model\n",
    "        self.model = model\n",
    "\n",
    "        # implement Laplace approximation\n",
    "        self.posterior_mean = ? \n",
    "        self.posterior_hessian = ? \n",
    "        self.posterior_cov = ? \n",
    "\n",
    "        # sanity check for dimensions\n",
    "        assert self.posterior_mean.shape == (model.D,), f\"The shape of the posterior mean appears wrong. Check your implementaion.\"\n",
    "        assert self.posterior_hessian.shape == (model.D, model.D), f\"The shape of the posterior Hessian appears wrong. Check your implementaion.\"\n",
    "        assert self.posterior_cov.shape == (model.D, model.D), f\"The shape of the posterior covariance appears wrong. Check your implementaion.\"\n",
    "\n",
    "    def log_pdf(self, w):\n",
    "        \"\"\" evaluate approximate posterior density at w \"\"\"\n",
    "        return mvn.logpdf(w, self.posterior_mean.ravel(), self.posterior_cov)\n",
    "    \n",
    "    def posterior_samples(self, num_samples):\n",
    "        \"\"\" generate samples from posterior distribution \"\"\"\n",
    "        return np.random.multivariate_normal(self.posterior_mean, self.posterior_cov, size=(num_samples))\n",
    "\n",
    "# prep model and approximation\n",
    "model = LogisticRegression(Xtrain, ytrain)\n",
    "laplace = LaplaceApproximation(model)\n",
    "\n",
    "##############################################\n",
    "# Your solution goes here\n",
    "##############################################\n",
    "##############################################\n",
    "# End of solution\n",
    "##############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.2**: Report the (approximate) posterior mean and covariance of $p(\\mathbf{w}|\\mathbf{y})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5:  Approximating the posterior predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained our Laplace approximation, i.e. $p(\\textbf{w}|\\textbf{y}) \\approx q(\\textbf{w}) = \\mathcal{N}(\\mathbf{w}|\\mathbf{m}, \\mathbf{S})$, it is finally time to make predictions. Our goal is to approximate the posterior predictive distribution $p(y^* = 1|\\mathbf{y}, \\mathbf{x}^*)$, which can be approximated as\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y^*=1|\\textbf{y}, \\textbf{x}^*) = \\int p(y^*=1|\\textbf{w}, \\textbf{x}^*) p(\\textbf{w}|\\textbf{y}) \\text{d}\\textbf{w} \\approx \\int p(y^*=1|\\textbf{w}, \\textbf{x}^*) q(\\textbf{w}) \\text{d}\\textbf{w} = \\int \\sigma(\\mathbf{w}^T \\mathbf{x}^*) \\mathcal{N}(\\mathbf{w}|\\mathbf{m}, \\mathbf{S}) \\text{d} \\mathbf{w} = \\int \\sigma(f^*) \\mathcal{N}(f^*|m^*, v^*) \\text{d} f^*,\n",
    "\\end{align*}$$\n",
    "\n",
    "where $m^* = \\mathbf{m}^T \\mathbf{x} ^*$ and  and $v^* = (\\mathbf{x}^*)^T  \\mathbf{S} \\mathbf{x}_*$. In this exercise, we will implement three options for approximating the above integral:\n",
    "\n",
    "**The plug-in approximation**\n",
    "\n",
    "The first option is the **plug-in** aproximation using $\\mathbf{m} = \\mathbf{w}_{\\text{MAP}}$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y^*=1|\\textbf{y}, \\textbf{x}^*) \\approx \\sigma(m^*)\n",
    "\\end{align*}$$\n",
    "\n",
    "This option is simple and very fast, but the drawback is that it ignores any posterior uncertainty for $f^*$.\n",
    "\n",
    "**The probit approximation**\n",
    "\n",
    "The second approach we will consider is the so-called **probit** approximation:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y^*=1|\\textbf{y}, \\textbf{x}^*) \\approx \\int \\sigma(f^*) \\mathcal{N}(f^*|m^*, v^*) \\text{d} f^* = \\int \\Phi(f^* \\sqrt{\\frac{\\pi}{8}}) \\mathcal{N}(f^*|m^*, v^*) \\text{d} f^* = \\Phi\\left(\\frac{m^*}{\\sqrt{\\frac{8}{\\pi} + v^*}}\\right),\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\Phi(\\cdot): \\mathbb{R} \\rightarrow (0,1)$ is the CDF of a standardized Gaussian distribution. The option is also fast, but comes with a small bias.\n",
    "\n",
    "\n",
    "**The Monte Carlo estimator**\n",
    "\n",
    "The third and last option is a Monte Carlo estimator:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y^*=1|\\textbf{y}, \\textbf{x}^*) \\approx \\frac{1}{S} \\sum_{i=1}^S \\sigma(f^*_{(i)}), \\quad \\text{where} \\quad \\sigma(f^*_{(i)}) \\sim \\mathcal{N}(f^*|m^*, v^*) \\quad \\text{for} \\quad i = 1, ..., S\n",
    "\\end{align*}$$\n",
    "\n",
    "We can make this estimator as precise as we want by increasing the number of Monte Carlo samples $S$. Of course, there is a trade between precision and computational cost. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.1**: Completet the implementation of the functions *montecarlo* and *probit_approx* in the class below for computing the Monte Carlo estimator and the probit approximation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probit = lambda x: norm_dist.cdf(x)\n",
    "\n",
    "class PosteriorPredictiveDistribution(object):\n",
    "    \n",
    "    def  __init__(self, model):\n",
    "        self.model = model\n",
    "        self.feature_transformation = model.feature_transformation\n",
    "        self.laplace = LaplaceApproximation(model)\n",
    "\n",
    "    def posterior_f(self, xstar_):\n",
    "        \"\"\" computes the mean and variance of f^* = w^T x^* \"\"\"\n",
    "        xstar = self.model.preprocess(xstar_)\n",
    "        m = xstar@self.laplace.posterior_mean\n",
    "        v = np.diag(xstar@self.laplace.posterior_cov@xstar.T)\n",
    "        return m, v\n",
    "\n",
    "    def plugin_approx(self, xstar_):\n",
    "        \"\"\" implements the plugin approximation for p(y^*|y, x^*) using w_MAP. If xstar has shape (M, D), then the shape of the output p must be (M,) \"\"\"\n",
    "        xstar = self.model.preprocess(xstar_)\n",
    "        p = model.predict(xstar, model.w_MAP)\n",
    "\n",
    "        assert p.shape == (len(xstar_),), f\"Expected the shape of the output from the Monte Carlo approximation to be ({len(xstar)},), but the received shape was {p.shape}\"\n",
    "        return p\n",
    "    \n",
    "    def montecarlo(self, xstar, num_samples=1000):\n",
    "        \"\"\" implements the Monte Carlo estimator for p(y^*|y, x^*). If xstar has shape (M, D), then the shape of the output p must be (M,) \"\"\"\n",
    "        m, v = self.posterior_f(xstar)\n",
    "\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "\n",
    "        assert p.shape == (len(xstar),), f\"Expected the shape of the output from the Monte Carlo approximation to be ({len(xstar)},), but the received shape was {p.shape}\"\n",
    "        return p\n",
    "\n",
    "    def probit_approx(self, xstar):\n",
    "        \"\"\" implements the probit approximation for p(y^*|y, x^*). If xstar has shape (M, D), then the shape of the output p must be (M,) \"\"\"\n",
    "        m, v = self.posterior_f(xstar)\n",
    "\n",
    "        ##############################################\n",
    "        # Your solution goes here\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # End of solution\n",
    "        ##############################################\n",
    "\n",
    "        assert p.shape == (len(xstar),), f\"Expected the shape of the output from the Monte Carlo approximation to be ({len(xstar)},), but the received shape was {p.shape}\"\n",
    "        return p\n",
    "    \n",
    "# simple polymials as feature transformation\n",
    "def feature_transformation(x, order=1):\n",
    "    return np.column_stack([x**m for m in range(order+1)])\n",
    "\n",
    "# set up model and posterior predictive distribution\n",
    "model = LogisticRegression(Xtrain, ytrain, feature_transformation=lambda x: feature_transformation(x, 3))\n",
    "postpred = PosteriorPredictiveDistribution(model)\n",
    "\n",
    "pred_dict = {   'Plug-in approx': postpred.plugin_approx,\n",
    "                'Probit approx':  postpred.probit_approx,\n",
    "                'Monte Carlo':    postpred.montecarlo,\n",
    "            }\n",
    "\n",
    "\n",
    "dim1 = np.linspace(Xtest[:, 0].min()-0.1, Xtest[:, 0].max()+0.1, 100)\n",
    "dim2 = np.linspace(Xtest[:, 1].min()-0.1, Xtest[:, 1].max()+0.1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "for i, (name, predict_func) in enumerate(pred_dict.items()):\n",
    "    predict_grid = Grid2D(dim1, dim2, predict_func)\n",
    "\n",
    "    plot_data(ax[0, i], Xtrain, ytrain, alpha=0.5)\n",
    "    plot_data(ax[1, i], Xtest, ytest, alpha=0.5)\n",
    "    predict_grid.plot_heatmap(ax[0, i])\n",
    "    predict_grid.plot_heatmap(ax[1, i])\n",
    "    ax[0, i].set_title(f'Training set ({name})', fontweight='bold')\n",
    "    ax[1, i].set_title(f'Test set ({name})', fontweight='bold')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2**: Study the maps of the posterior predictive probability above and comment and the qualitative differences [**Discussion question**]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.3**: Compute the training and test accuracy and ELPD for each of the three approximations:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6:  Insights into the posterior predictive distribution via the probit approximation\n",
    "\n",
    "Besides being a very and accurate approximation of the posterior predictive distribution, we also use the probit approximation to gain insight into the behaviour of the model. The following task shows that if the posterior uncertainty of the parameters becomes sufficiently, i.e. $v^* = (\\mathbf{x}^*)^T  \\mathbf{S} \\mathbf{x}_*$, then the posterior predictive distribution will be approach $\\frac{1}{2}$.\n",
    "\n",
    "\n",
    "**Task 6.1**: Show that the posterior predictive probability $p(y^*=1|\\textbf{y}, \\mathbf{x}) \\rightarrow 0.5$ as the variance $v^* \\rightarrow \\infty$.\n",
    "\n",
    "*Hints: What happens to $\\frac{m^*}{\\sqrt{\\frac{8}{\\pi} + v^*}}$ as $v^*$ increases?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 6.2**: Show/argue that the first two options generate the same **decisions** if we use the decision rule $y^* = \\arg\\max_{k} p(y^*=k|\\textbf{y}, \\textbf{x}^*)$.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02477",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
